{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:29.557564Z",
     "iopub.status.busy": "2020-09-27T22:21:29.556852Z",
     "iopub.status.idle": "2020-09-27T22:21:35.138759Z",
     "shell.execute_reply": "2020-09-27T22:21:35.137771Z"
    },
    "papermill": {
     "duration": 5.601613,
     "end_time": "2020-09-27T22:21:35.138876",
     "exception": false,
     "start_time": "2020-09-27T22:21:29.537263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import random \n",
    "import os\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from sklearn import ensemble\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "from tqdm import tqdm \n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:35.168858Z",
     "iopub.status.busy": "2020-09-27T22:21:35.168175Z",
     "iopub.status.idle": "2020-09-27T22:21:35.170731Z",
     "shell.execute_reply": "2020-09-27T22:21:35.171178Z"
    },
    "papermill": {
     "duration": 0.019349,
     "end_time": "2020-09-27T22:21:35.171280",
     "exception": false,
     "start_time": "2020-09-27T22:21:35.151931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path='hopefully_model.hope'\n",
    "check_point=ModelCheckpoint(file_path,monitor='loss',verbose=1,save_best_only=True,mode='min')\n",
    "callbacks=[check_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:35.199952Z",
     "iopub.status.busy": "2020-09-27T22:21:35.199417Z",
     "iopub.status.idle": "2020-09-27T22:21:36.169550Z",
     "shell.execute_reply": "2020-09-27T22:21:36.168145Z"
    },
    "papermill": {
     "duration": 0.985733,
     "end_time": "2020-09-27T22:21:36.169700",
     "exception": false,
     "start_time": "2020-09-27T22:21:35.183967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../input/segmented-data-1/ct_data.pkl\",'rb')as f:\n",
    "    data_ct=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:36.208016Z",
     "iopub.status.busy": "2020-09-27T22:21:36.207117Z",
     "iopub.status.idle": "2020-09-27T22:21:36.373515Z",
     "shell.execute_reply": "2020-09-27T22:21:36.372487Z"
    },
    "papermill": {
     "duration": 0.190709,
     "end_time": "2020-09-27T22:21:36.373664",
     "exception": false,
     "start_time": "2020-09-27T22:21:36.182955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT = \"../input/osic-pulmonary-fibrosis-progression\"\n",
    "tr = pd.read_csv(f\"{ROOT}/train.csv\")\n",
    "tr.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\n",
    "tr['min_week'] = tr['Weeks']\n",
    "tr['min_week'] = tr.groupby('Patient')['min_week'].transform('min')\n",
    "base = tr.loc[tr.Weeks == tr.min_week]\n",
    "base = base[['Patient','FVC']].copy()\n",
    "base.columns = ['Patient','min_FVC']\n",
    "base['nb'] = 1\n",
    "base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n",
    "base = base[base.nb==1]\n",
    "base.drop('nb', axis=1, inplace=True)\n",
    "tr = tr.merge(base, on='Patient', how='left')\n",
    "tr['base_week'] = tr['Weeks'] - tr['min_week']\n",
    "del base\n",
    "gc.collect()\n",
    "COLS = ['Sex','SmokingStatus'] #,'Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:36.407128Z",
     "iopub.status.busy": "2020-09-27T22:21:36.406339Z",
     "iopub.status.idle": "2020-09-27T22:21:36.539352Z",
     "shell.execute_reply": "2020-09-27T22:21:36.538906Z"
    },
    "papermill": {
     "duration": 0.152859,
     "end_time": "2020-09-27T22:21:36.539454",
     "exception": false,
     "start_time": "2020-09-27T22:21:36.386595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "gender=LabelBinarizer().fit(tr[['Sex']])\n",
    "tr['gender']=gender.transform(tr[['Sex']])\n",
    "smkstat=LabelBinarizer()\n",
    "ww=smkstat.fit_transform(tr.SmokingStatus)\n",
    "df_smk=pd.DataFrame(ww,columns=smkstat.classes_)\n",
    "tr=tr.merge(df_smk,how='left',left_index=True,right_index=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:36.573895Z",
     "iopub.status.busy": "2020-09-27T22:21:36.573236Z",
     "iopub.status.idle": "2020-09-27T22:21:36.576510Z",
     "shell.execute_reply": "2020-09-27T22:21:36.576907Z"
    },
    "papermill": {
     "duration": 0.022917,
     "end_time": "2020-09-27T22:21:36.577035",
     "exception": false,
     "start_time": "2020-09-27T22:21:36.554118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_percentage(tr):\n",
    "    df=tr.copy()\n",
    "    pateints=df.loc[:,'Patient'].astype(str).unique()\n",
    "    for n, pat in enumerate(pateints):\n",
    "        percent=df.loc[(df.Patient.astype(str)==pat)&(df.base_week==0),\"Percent\"].values[0]\n",
    "        df.loc[df.Patient.astype(str)==pat,'Percent']=percent\n",
    "        print(n)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:36.607517Z",
     "iopub.status.busy": "2020-09-27T22:21:36.606778Z",
     "iopub.status.idle": "2020-09-27T22:21:37.125080Z",
     "shell.execute_reply": "2020-09-27T22:21:37.126630Z"
    },
    "papermill": {
     "duration": 0.53658,
     "end_time": "2020-09-27T22:21:37.126813",
     "exception": false,
     "start_time": "2020-09-27T22:21:36.590233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "tr=set_percentage(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:37.284691Z",
     "iopub.status.busy": "2020-09-27T22:21:37.165686Z",
     "iopub.status.idle": "2020-09-27T22:21:37.288107Z",
     "shell.execute_reply": "2020-09-27T22:21:37.288536Z"
    },
    "papermill": {
     "duration": 0.146832,
     "end_time": "2020-09-27T22:21:37.288691",
     "exception": false,
     "start_time": "2020-09-27T22:21:37.141859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=[\"Weeks\",\"FVC\",\"Percent\",\"Age\",\"min_week\",\"min_FVC\",\"base_week\"]\n",
    "cols_norm=MinMaxScaler()\n",
    "norm=cols_norm.fit_transform(tr[cols])\n",
    "norm_df=pd.DataFrame(norm,columns=[new_name+'_norm' for new_name in cols ])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:37.326226Z",
     "iopub.status.busy": "2020-09-27T22:21:37.324889Z",
     "iopub.status.idle": "2020-09-27T22:21:37.327931Z",
     "shell.execute_reply": "2020-09-27T22:21:37.327464Z"
    },
    "papermill": {
     "duration": 0.024105,
     "end_time": "2020-09-27T22:21:37.328025",
     "exception": false,
     "start_time": "2020-09-27T22:21:37.303920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr=tr.merge(norm_df,how='right',left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:37.364516Z",
     "iopub.status.busy": "2020-09-27T22:21:37.363695Z",
     "iopub.status.idle": "2020-09-27T22:21:37.397050Z",
     "shell.execute_reply": "2020-09-27T22:21:37.397506Z"
    },
    "papermill": {
     "duration": 0.054935,
     "end_time": "2020-09-27T22:21:37.397635",
     "exception": false,
     "start_time": "2020-09-27T22:21:37.342700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Weeks</th>\n",
       "      <th>FVC</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SmokingStatus</th>\n",
       "      <th>min_week</th>\n",
       "      <th>min_FVC</th>\n",
       "      <th>base_week</th>\n",
       "      <th>...</th>\n",
       "      <th>Currently smokes</th>\n",
       "      <th>Ex-smoker</th>\n",
       "      <th>Never smoked</th>\n",
       "      <th>Weeks_norm</th>\n",
       "      <th>FVC_norm</th>\n",
       "      <th>Percent_norm</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>min_week_norm</th>\n",
       "      <th>min_FVC_norm</th>\n",
       "      <th>base_week_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>-4</td>\n",
       "      <td>2315</td>\n",
       "      <td>58.253649</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-4</td>\n",
       "      <td>2315</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.267050</td>\n",
       "      <td>0.135886</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.241456</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>5</td>\n",
       "      <td>2214</td>\n",
       "      <td>58.253649</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-4</td>\n",
       "      <td>2315</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.248923</td>\n",
       "      <td>0.135886</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.241456</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>7</td>\n",
       "      <td>2061</td>\n",
       "      <td>58.253649</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-4</td>\n",
       "      <td>2315</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.221464</td>\n",
       "      <td>0.135886</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.241456</td>\n",
       "      <td>0.174603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>9</td>\n",
       "      <td>2144</td>\n",
       "      <td>58.253649</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-4</td>\n",
       "      <td>2315</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.236360</td>\n",
       "      <td>0.135886</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.241456</td>\n",
       "      <td>0.206349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>11</td>\n",
       "      <td>2069</td>\n",
       "      <td>58.253649</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-4</td>\n",
       "      <td>2315</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.135886</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.241456</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>ID00426637202313170790466</td>\n",
       "      <td>13</td>\n",
       "      <td>2712</td>\n",
       "      <td>71.824968</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Never smoked</td>\n",
       "      <td>0</td>\n",
       "      <td>2925</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.338299</td>\n",
       "      <td>0.259644</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.354755</td>\n",
       "      <td>0.206349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>ID00426637202313170790466</td>\n",
       "      <td>19</td>\n",
       "      <td>2978</td>\n",
       "      <td>71.824968</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Never smoked</td>\n",
       "      <td>0</td>\n",
       "      <td>2925</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.386037</td>\n",
       "      <td>0.259644</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.354755</td>\n",
       "      <td>0.301587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>ID00426637202313170790466</td>\n",
       "      <td>31</td>\n",
       "      <td>2908</td>\n",
       "      <td>71.824968</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Never smoked</td>\n",
       "      <td>0</td>\n",
       "      <td>2925</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.373475</td>\n",
       "      <td>0.259644</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.354755</td>\n",
       "      <td>0.492063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>ID00426637202313170790466</td>\n",
       "      <td>43</td>\n",
       "      <td>2975</td>\n",
       "      <td>71.824968</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Never smoked</td>\n",
       "      <td>0</td>\n",
       "      <td>2925</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.385499</td>\n",
       "      <td>0.259644</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.354755</td>\n",
       "      <td>0.682540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>ID00426637202313170790466</td>\n",
       "      <td>59</td>\n",
       "      <td>2774</td>\n",
       "      <td>71.824968</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Never smoked</td>\n",
       "      <td>0</td>\n",
       "      <td>2925</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.349426</td>\n",
       "      <td>0.259644</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.354755</td>\n",
       "      <td>0.936508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1535 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Patient  Weeks   FVC    Percent  Age   Sex  \\\n",
       "0     ID00007637202177411956430     -4  2315  58.253649   79  Male   \n",
       "1     ID00007637202177411956430      5  2214  58.253649   79  Male   \n",
       "2     ID00007637202177411956430      7  2061  58.253649   79  Male   \n",
       "3     ID00007637202177411956430      9  2144  58.253649   79  Male   \n",
       "4     ID00007637202177411956430     11  2069  58.253649   79  Male   \n",
       "...                         ...    ...   ...        ...  ...   ...   \n",
       "1530  ID00426637202313170790466     13  2712  71.824968   73  Male   \n",
       "1531  ID00426637202313170790466     19  2978  71.824968   73  Male   \n",
       "1532  ID00426637202313170790466     31  2908  71.824968   73  Male   \n",
       "1533  ID00426637202313170790466     43  2975  71.824968   73  Male   \n",
       "1534  ID00426637202313170790466     59  2774  71.824968   73  Male   \n",
       "\n",
       "     SmokingStatus  min_week  min_FVC  base_week  ...  Currently smokes  \\\n",
       "0        Ex-smoker        -4     2315          0  ...                 0   \n",
       "1        Ex-smoker        -4     2315          9  ...                 0   \n",
       "2        Ex-smoker        -4     2315         11  ...                 0   \n",
       "3        Ex-smoker        -4     2315         13  ...                 0   \n",
       "4        Ex-smoker        -4     2315         15  ...                 0   \n",
       "...            ...       ...      ...        ...  ...               ...   \n",
       "1530  Never smoked         0     2925         13  ...                 0   \n",
       "1531  Never smoked         0     2925         19  ...                 0   \n",
       "1532  Never smoked         0     2925         31  ...                 0   \n",
       "1533  Never smoked         0     2925         43  ...                 0   \n",
       "1534  Never smoked         0     2925         59  ...                 0   \n",
       "\n",
       "      Ex-smoker  Never smoked  Weeks_norm  FVC_norm  Percent_norm  Age_norm  \\\n",
       "0             1             0    0.007246  0.267050      0.135886  0.769231   \n",
       "1             1             0    0.072464  0.248923      0.135886  0.769231   \n",
       "2             1             0    0.086957  0.221464      0.135886  0.769231   \n",
       "3             1             0    0.101449  0.236360      0.135886  0.769231   \n",
       "4             1             0    0.115942  0.222900      0.135886  0.769231   \n",
       "...         ...           ...         ...       ...           ...       ...   \n",
       "1530          0             1    0.130435  0.338299      0.259644  0.615385   \n",
       "1531          0             1    0.173913  0.386037      0.259644  0.615385   \n",
       "1532          0             1    0.260870  0.373475      0.259644  0.615385   \n",
       "1533          0             1    0.347826  0.385499      0.259644  0.615385   \n",
       "1534          0             1    0.463768  0.349426      0.259644  0.615385   \n",
       "\n",
       "      min_week_norm  min_FVC_norm  base_week_norm  \n",
       "0          0.011905      0.241456        0.000000  \n",
       "1          0.011905      0.241456        0.142857  \n",
       "2          0.011905      0.241456        0.174603  \n",
       "3          0.011905      0.241456        0.206349  \n",
       "4          0.011905      0.241456        0.238095  \n",
       "...             ...           ...             ...  \n",
       "1530       0.059524      0.354755        0.206349  \n",
       "1531       0.059524      0.354755        0.301587  \n",
       "1532       0.059524      0.354755        0.492063  \n",
       "1533       0.059524      0.354755        0.682540  \n",
       "1534       0.059524      0.354755        0.936508  \n",
       "\n",
       "[1535 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:39.726111Z",
     "iopub.status.busy": "2020-09-27T22:21:37.437484Z",
     "iopub.status.idle": "2020-09-27T22:21:39.729985Z",
     "shell.execute_reply": "2020-09-27T22:21:39.729492Z"
    },
    "papermill": {
     "duration": 2.315815,
     "end_time": "2020-09-27T22:21:39.730107",
     "exception": false,
     "start_time": "2020-09-27T22:21:37.414292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n",
    "def qloss(y_true, y_pred):\n",
    "    # Pinball loss for multiple quantiles\n",
    "    qs = [0.2, 0.50, 0.8]\n",
    "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q*e, (q-1)*e)\n",
    "    return K.mean(v)\n",
    "\n",
    "def mloss(_lambda):\n",
    "    def loss(y_true, y_pred):\n",
    "        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    tf.dtypes.cast(y_true, tf.float32)\n",
    "    tf.dtypes.cast(y_pred, tf.float32)\n",
    "    sigma = y_pred[:, 2] - y_pred[:, 0]\n",
    "    fvc_pred = y_pred[:, 1]\n",
    "    \n",
    "    #sigma_clip = sigma + C1\n",
    "    sigma_clip = tf.maximum(sigma, C1)\n",
    "    delta = tf.abs(y_true[:, 0] - fvc_pred)\n",
    "    delta = tf.minimum(delta, C2)\n",
    "    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n",
    "    metric = -(delta / sigma_clip)*sq2 -tf.math.log(sigma_clip* sq2)\n",
    "    return K.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:39.781462Z",
     "iopub.status.busy": "2020-09-27T22:21:39.780272Z",
     "iopub.status.idle": "2020-09-27T22:21:39.783078Z",
     "shell.execute_reply": "2020-09-27T22:21:39.782474Z"
    },
    "papermill": {
     "duration": 0.036622,
     "end_time": "2020-09-27T22:21:39.783175",
     "exception": false,
     "start_time": "2020-09-27T22:21:39.746553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def get_patient_ct(data_ct,pateint_id):\n",
    "    for n,dic in enumerate(data_ct):\n",
    "        if dic['ID']==pateint_id:\n",
    "            return n\n",
    "    return None\n",
    "def MiniBatch(CTs,tabular,BatchN=0,n=64):\n",
    "    ct=[]\n",
    "    ind=[]\n",
    "    tab=tabular[BatchN*n:BatchN*n+n]\n",
    "    tab=tab.reset_index()\n",
    "    for i in range(n):\n",
    "        ind=get_patient_ct(CTs,tab[\"Patient\"].iloc[i])\n",
    "        ct.append(CTs[ind][\"cts\"])\n",
    "    ct=np.array(ct)\n",
    "    gc.collect()\n",
    "    return ct,tab\n",
    "def training(CTs,Tabular,epochs=10,batch_size=32):\n",
    "    FE=[\"Percent_norm\",\"Age_norm\",\"min_FVC_norm\",\"base_week_norm\",'gender','Currently smokes','Ex-smoker','Never smoked']\n",
    "    TabularLentgh=len(Tabular)\n",
    "    Tabular=shuffle(Tabular)\n",
    "    Tabular=Tabular.reset_index(drop=True)\n",
    "    Number_of_Batchs=int(TabularLentgh/batch_size)\n",
    "    for x in range (epochs):\n",
    "        print(\"epoch : \", x)\n",
    "        for i in range(Number_of_Batchs):\n",
    "            print(f\"batch {i} outta {Number_of_Batchs}\")\n",
    "            cts , tabular = MiniBatch(CTs,Tabular,i,batch_size)\n",
    "            Y=tabular.FVC\n",
    "            X=tabular[FE]\n",
    "            X=X.values\n",
    "            X=tf.constant(X, dtype=tf.float64)\n",
    "            Y=tf.constant(Y.values, dtype=tf.float64)\n",
    "            cts=cts.reshape(batch_size,32,256,256,1)\n",
    "            model.fit(x=[X,cts],y=Y,batch_size=4,callbacks=callbacks)\n",
    "        if(TabularLentgh%batch_size != 0  ):\n",
    "            print(\"last batch\")\n",
    "            cts , tabular = MiniBatch(CTs,Tabular,Number_of_Batchs,TabularLentgh-batch_size*Number_of_Batchs)\n",
    "            Y=tabular.FVC\n",
    "            X=tabular[FE]\n",
    "            X=X.values\n",
    "            X=tf.constant(X, dtype=tf.float64)\n",
    "            Y=tf.constant(Y.values, dtype=tf.float64)\n",
    "            cts=cts.reshape(TabularLentgh-batch_size*Number_of_Batchs,32,256,256,1)\n",
    "            model.fit(x=[X,cts],y=Y,batch_size=1,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:39.820330Z",
     "iopub.status.busy": "2020-09-27T22:21:39.819737Z",
     "iopub.status.idle": "2020-09-27T22:21:40.806555Z",
     "shell.execute_reply": "2020-09-27T22:21:40.805716Z"
    },
    "papermill": {
     "duration": 1.007515,
     "end_time": "2020-09-27T22:21:40.806731",
     "exception": false,
     "start_time": "2020-09-27T22:21:39.799216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "def norm_cts(cts):\n",
    "    norm_cts=(cts+2740)/(2740)\n",
    "    return norm_cts\n",
    "def norm_all_cts():\n",
    "    for n,data in enumerate(data_ct):\n",
    "        temp_cts=norm_cts(data[\"cts\"])\n",
    "        data['cts']=temp_cts\n",
    "        print(n)\n",
    "norm_all_cts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:40.867072Z",
     "iopub.status.busy": "2020-09-27T22:21:40.865876Z",
     "iopub.status.idle": "2020-09-27T22:21:41.392024Z",
     "shell.execute_reply": "2020-09-27T22:21:41.391197Z"
    },
    "papermill": {
     "duration": 0.566272,
     "end_time": "2020-09-27T22:21:41.392181",
     "exception": false,
     "start_time": "2020-09-27T22:21:40.825909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "right_input (InputLayer)        [(None, 32, 256, 256 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_branch (Conv3D)           (None, 32, 256, 256, 144         right_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "right_branch2 (MaxPooling3D)    (None, 16, 128, 128, 0           right_branch[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "drop1 (Dropout)                 (None, 16, 128, 128, 0           right_branch2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "right_branch3 (Conv3D)          (None, 16, 128, 128, 2064        drop1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 128, 128, 64          right_branch3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "right_branch4 (MaxPooling3D)    (None, 8, 64, 64, 16 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "right_branch5 (Conv3D)          (None, 8, 64, 64, 32 4128        right_branch4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "right_branch6 (MaxPooling3D)    (None, 4, 32, 32, 32 0           right_branch5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "right_branch7 (Conv3D)          (None, 4, 32, 32, 64 16448       right_branch6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "drop2 (Dropout)                 (None, 4, 32, 32, 64 0           right_branch7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4, 32, 32, 64 256         drop2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "right_branch8 (MaxPooling3D)    (None, 2, 16, 16, 64 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "left_input (InputLayer)         [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_branch9 (Conv3D)          (None, 2, 16, 16, 12 65664       right_branch8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "left_branch (Dense)             (None, 16)           144         left_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "drop3 (Dropout)                 (None, 2, 16, 16, 12 0           right_branch9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "d2 (Dense)                      (None, 25)           425         left_branch[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 65536)        0           drop3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "d3 (Dense)                      (None, 100)          2600        d2[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50)           3276850     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 150)          0           d3[0][0]                         \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          15100       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "main_ (Dense)                   (None, 10)           1010        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p1 (Dense)                      (None, 3)            33          main_[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "p2 (Dense)                      (None, 3)            33          main_[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "preds (Lambda)                  (None, 3)            0           p1[0][0]                         \n",
      "                                                                 p2[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 3,384,963\n",
      "Trainable params: 3,384,803\n",
      "Non-trainable params: 160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate,Add\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "left_input = Input(shape=(8, ), name='left_input')\n",
    "left_branch = Dense(16, input_dim=8, name='left_branch')(left_input)\n",
    "left_branch= L.Dense(25, activation=\"relu\", name=\"d2\")(left_branch)\n",
    "left_branch= L.Dense(100, activation=\"relu\", name=\"d3\")(left_branch)\n",
    "\n",
    "\n",
    "right_input = Input(shape=(32,256,256,1), name='right_input')\n",
    "right_branch = Conv3D(16, (2,2,2), activation='relu',padding='same' , input_shape=(32,256,256), name='right_branch')(right_input)\n",
    "right_branch = MaxPooling3D((2,2,2),padding='same' , name='right_branch2')(right_branch)\n",
    "right_branch=Dropout(0.2,name=\"drop1\")(right_branch)\n",
    "right_branch = Conv3D(16, (2,2,2), activation='relu',padding='same' , name='right_branch3')(right_branch)\n",
    "right_branch=BatchNormalization()(right_branch)\n",
    "right_branch = MaxPooling3D((2,2,2),padding='same' , name='right_branch4')(right_branch)\n",
    "right_branch = Conv3D(32, (2,2,2), activation='relu',padding='same' , name='right_branch5')(right_branch)\n",
    "right_branch = MaxPooling3D((2,2,2),padding='same' , name='right_branch6')(right_branch)\n",
    "right_branch = Conv3D(64, (2,2,2), activation='relu',padding='same' , name='right_branch7')(right_branch)\n",
    "right_branch=Dropout(0.36,name=\"drop2\")(right_branch)\n",
    "right_branch=BatchNormalization()(right_branch)\n",
    "right_branch = MaxPooling3D((2,2,2),padding='same' , name='right_branch8')(right_branch)\n",
    "right_branch = Conv3D(128, (2,2,2), activation='relu',padding='same' , name='right_branch9')(right_branch)\n",
    "right_branch=Dropout(0.36,name=\"drop3\")(right_branch)\n",
    "right_branch = Flatten()(right_branch)\n",
    "right_branch=Dense(50)(right_branch)\n",
    "x = concatenate([left_branch, right_branch])\n",
    "a=Dense(100,activation=\"relu\")(x)\n",
    "a=Dense(10, activation='relu', name='main_')(a)\n",
    "p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(a)\n",
    "p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(a)\n",
    "preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n",
    "                 name=\"preds\")([p1, p2])\n",
    "\n",
    "model = Model(inputs=[left_input, right_input], outputs=preds)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False),loss=mloss(0.8),metrics=[score])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:21:41.434086Z",
     "iopub.status.busy": "2020-09-27T22:21:41.432897Z",
     "iopub.status.idle": "2020-09-27T22:56:52.421951Z",
     "shell.execute_reply": "2020-09-27T22:56:52.408643Z"
    },
    "papermill": {
     "duration": 2111.010993,
     "end_time": "2020-09-27T22:56:52.422135",
     "exception": false,
     "start_time": "2020-09-27T22:21:41.411142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 553.3504 - score: -9.5625\n",
      "Epoch 00001: loss improved from inf to 553.35040, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 377ms/step - loss: 553.3504 - score: -9.5625\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 271.8628 - score: -8.2329\n",
      "Epoch 00001: loss improved from 553.35040 to 271.86282, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 405ms/step - loss: 271.8628 - score: -8.2329\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 232.1063 - score: -8.2647\n",
      "Epoch 00001: loss improved from 271.86282 to 232.10628, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 364ms/step - loss: 232.1063 - score: -8.2647\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 273.8722 - score: -8.2342\n",
      "Epoch 00001: loss did not improve from 232.10628\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 273.8722 - score: -8.2342\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 252.3905 - score: -8.3213\n",
      "Epoch 00001: loss did not improve from 232.10628\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 252.3905 - score: -8.3213\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 199.9594 - score: -8.2167\n",
      "Epoch 00001: loss improved from 232.10628 to 199.95937, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 392ms/step - loss: 199.9594 - score: -8.2167\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 219.0479 - score: -8.2070\n",
      "Epoch 00001: loss did not improve from 199.95937\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 219.0479 - score: -8.2070\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 145.3591 - score: -7.9440\n",
      "Epoch 00001: loss improved from 199.95937 to 145.35907, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 374ms/step - loss: 145.3591 - score: -7.9440\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 132.9052 - score: -7.7528\n",
      "Epoch 00001: loss improved from 145.35907 to 132.90517, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 359ms/step - loss: 132.9052 - score: -7.7528\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 115.1027 - score: -7.6264\n",
      "Epoch 00001: loss improved from 132.90517 to 115.10273, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 368ms/step - loss: 115.1027 - score: -7.6264\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 128.0354 - score: -7.7170\n",
      "Epoch 00001: loss did not improve from 115.10273\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 128.0354 - score: -7.7170\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 119.1114 - score: -7.7431\n",
      "Epoch 00001: loss did not improve from 115.10273\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 119.1114 - score: -7.7431\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 110.3302 - score: -7.6646\n",
      "Epoch 00001: loss improved from 115.10273 to 110.33023, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 368ms/step - loss: 110.3302 - score: -7.6646\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 114.9695 - score: -7.6038\n",
      "Epoch 00001: loss did not improve from 110.33023\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 114.9695 - score: -7.6038\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 101.6243 - score: -7.6307\n",
      "Epoch 00001: loss improved from 110.33023 to 101.62430, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 362ms/step - loss: 101.6243 - score: -7.6307\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 94.8062 - score: -7.4679\n",
      "Epoch 00001: loss improved from 101.62430 to 94.80615, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 367ms/step - loss: 94.8062 - score: -7.4679\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 81.9306 - score: -7.3800\n",
      "Epoch 00001: loss improved from 94.80615 to 81.93065, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 386ms/step - loss: 81.9306 - score: -7.3800\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 86.3914 - score: -7.3255\n",
      "Epoch 00001: loss did not improve from 81.93065\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 86.3914 - score: -7.3255\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 78.9182 - score: -7.3013\n",
      "Epoch 00001: loss improved from 81.93065 to 78.91824, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 376ms/step - loss: 78.9182 - score: -7.3013\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 82.0954 - score: -7.3330\n",
      "Epoch 00001: loss did not improve from 78.91824\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 82.0954 - score: -7.3330\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 85.4629 - score: -7.4051\n",
      "Epoch 00001: loss did not improve from 78.91824\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 85.4629 - score: -7.4051\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 70.8731 - score: -7.2564\n",
      "Epoch 00001: loss improved from 78.91824 to 70.87312, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 373ms/step - loss: 70.8731 - score: -7.2564\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 74.0770 - score: -7.2570\n",
      "Epoch 00001: loss did not improve from 70.87312\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 74.0770 - score: -7.2570\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 94.4735 - score: -7.4271\n",
      "Epoch 00001: loss did not improve from 70.87312\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 94.4735 - score: -7.4271\n",
      "epoch :  1\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 70.6908 - score: -7.2094\n",
      "Epoch 00001: loss improved from 70.87312 to 70.69083, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 362ms/step - loss: 70.6908 - score: -7.2094\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 77.6155 - score: -7.2835\n",
      "Epoch 00001: loss did not improve from 70.69083\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 77.6155 - score: -7.2835\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 63.9167 - score: -7.1230\n",
      "Epoch 00001: loss improved from 70.69083 to 63.91665, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 372ms/step - loss: 63.9167 - score: -7.1230\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 77.0500 - score: -7.1910\n",
      "Epoch 00001: loss did not improve from 63.91665\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 77.0500 - score: -7.1910\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 82.7041 - score: -7.2442\n",
      "Epoch 00001: loss did not improve from 63.91665\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 82.7041 - score: -7.2442\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 68.0705 - score: -7.1111\n",
      "Epoch 00001: loss did not improve from 63.91665\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 68.0705 - score: -7.1111\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 70.5988 - score: -7.1484\n",
      "Epoch 00001: loss did not improve from 63.91665\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 70.5988 - score: -7.1484\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 63.6288 - score: -7.0186\n",
      "Epoch 00001: loss improved from 63.91665 to 63.62877, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 369ms/step - loss: 63.6288 - score: -7.0186\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 61.3574 - score: -7.0251\n",
      "Epoch 00001: loss improved from 63.62877 to 61.35740, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 7s 407ms/step - loss: 61.3574 - score: -7.0251\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 68.3112 - score: -7.1141\n",
      "Epoch 00001: loss did not improve from 61.35740\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 68.3112 - score: -7.1141\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 62.2407 - score: -7.0142\n",
      "Epoch 00001: loss did not improve from 61.35740\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 62.2407 - score: -7.0142\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 75.6845 - score: -7.1822\n",
      "Epoch 00001: loss did not improve from 61.35740\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 75.6845 - score: -7.1822\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 74.0447 - score: -7.1882\n",
      "Epoch 00001: loss did not improve from 61.35740\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 74.0447 - score: -7.1882\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 64.2198 - score: -7.1225\n",
      "Epoch 00001: loss did not improve from 61.35740\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 64.2198 - score: -7.1225\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 69.8184 - score: -7.1749\n",
      "Epoch 00001: loss did not improve from 61.35740\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 69.8184 - score: -7.1749\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 60.9758 - score: -7.0290\n",
      "Epoch 00001: loss improved from 61.35740 to 60.97581, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 383ms/step - loss: 60.9758 - score: -7.0290\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 62.3682 - score: -7.0382\n",
      "Epoch 00001: loss did not improve from 60.97581\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 62.3682 - score: -7.0382\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 69.7340 - score: -7.0879\n",
      "Epoch 00001: loss did not improve from 60.97581\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 69.7340 - score: -7.0879\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 65.6229 - score: -7.0702\n",
      "Epoch 00001: loss did not improve from 60.97581\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 65.6229 - score: -7.0702\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 60.3159 - score: -7.0025\n",
      "Epoch 00001: loss improved from 60.97581 to 60.31585, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 361ms/step - loss: 60.3159 - score: -7.0025\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 65.6984 - score: -7.0770\n",
      "Epoch 00001: loss did not improve from 60.31585\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 65.6984 - score: -7.0770\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.7081 - score: -6.9491\n",
      "Epoch 00001: loss improved from 60.31585 to 56.70809, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 389ms/step - loss: 56.7081 - score: -6.9491\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 66.7044 - score: -7.0551\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 66.7044 - score: -7.0551\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 75.8424 - score: -7.2450\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "63/63 [==============================] - 7s 116ms/step - loss: 75.8424 - score: -7.2450\n",
      "epoch :  2\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 67.1362 - score: -7.1242\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 67.1362 - score: -7.1242\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 75.4481 - score: -7.2327\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 75.4481 - score: -7.2327\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 66.8579 - score: -7.1132\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 66.8579 - score: -7.1132\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 64.1046 - score: -7.0325\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 64.1046 - score: -7.0325\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 68.3580 - score: -7.1041\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 68.3580 - score: -7.1041\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 62.0559 - score: -7.0261\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 62.0559 - score: -7.0261\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 65.8212 - score: -7.1057\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 65.8212 - score: -7.1057\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 60.8278 - score: -6.9813\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 60.8278 - score: -6.9813\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.8336 - score: -7.0017\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 58.8336 - score: -7.0017\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 65.1807 - score: -7.0145\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 65.1807 - score: -7.0145\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 61.0583 - score: -6.9821\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 61.0583 - score: -6.9821\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 66.8134 - score: -7.0865\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 66.8134 - score: -7.0865\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 64.0672 - score: -7.0645\n",
      "Epoch 00001: loss did not improve from 56.70809\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 64.0672 - score: -7.0645\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.8590 - score: -6.9974\n",
      "Epoch 00001: loss improved from 56.70809 to 54.85898, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 378ms/step - loss: 54.8590 - score: -6.9974\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 65.0721 - score: -7.0779\n",
      "Epoch 00001: loss did not improve from 54.85898\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 65.0721 - score: -7.0779\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.6685 - score: -6.9421\n",
      "Epoch 00001: loss did not improve from 54.85898\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 58.6685 - score: -6.9421\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.3725 - score: -6.9490\n",
      "Epoch 00001: loss did not improve from 54.85898\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 57.3725 - score: -6.9490\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 67.0251 - score: -7.0035\n",
      "Epoch 00001: loss did not improve from 54.85898\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 67.0251 - score: -7.0035\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 61.5391 - score: -7.0108\n",
      "Epoch 00001: loss did not improve from 54.85898\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 61.5391 - score: -7.0108\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.0687 - score: -6.9391\n",
      "Epoch 00001: loss did not improve from 54.85898\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 58.0687 - score: -6.9391\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 67.1844 - score: -7.0825\n",
      "Epoch 00001: loss did not improve from 54.85898\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 67.1844 - score: -7.0825\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.7156 - score: -6.8735\n",
      "Epoch 00001: loss improved from 54.85898 to 49.71564, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 358ms/step - loss: 49.7156 - score: -6.8735\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 63.4639 - score: -7.0073\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 63.4639 - score: -7.0073\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 78.1316 - score: -7.2907\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 78.1316 - score: -7.2907\n",
      "epoch :  3\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 65.7903 - score: -7.1027\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 65.7903 - score: -7.1027\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 64.1158 - score: -7.0388\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 64.1158 - score: -7.0388\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.7228 - score: -6.9619\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 56.7228 - score: -6.9619\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 61.8138 - score: -6.9544\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 61.8138 - score: -6.9544\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 68.4299 - score: -7.0776\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 68.4299 - score: -7.0776\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 61.2333 - score: -6.9804\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 61.2333 - score: -6.9804\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 60.8180 - score: -7.0133\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 60.8180 - score: -7.0133\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.4283 - score: -6.9161\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 57.4283 - score: -6.9161\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.2084 - score: -6.8801\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 55.2084 - score: -6.8801\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 64.2049 - score: -7.0171\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 64.2049 - score: -7.0171\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 59.2317 - score: -6.9581\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 59.2317 - score: -6.9581\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 65.2148 - score: -7.0670\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 65.2148 - score: -7.0670\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 61.3168 - score: -6.9929\n",
      "Epoch 00001: loss did not improve from 49.71564\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 61.3168 - score: -6.9929\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.6267 - score: -6.8517\n",
      "Epoch 00001: loss improved from 49.71564 to 48.62673, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 359ms/step - loss: 48.6267 - score: -6.8517\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 64.2213 - score: -7.0218\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 64.2213 - score: -7.0218\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.9461 - score: -6.9548\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 58.9461 - score: -6.9548\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.0952 - score: -6.9529\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 57.0952 - score: -6.9529\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 63.3223 - score: -6.9886\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 63.3223 - score: -6.9886\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 61.8648 - score: -6.9577\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 61.8648 - score: -6.9577\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.1580 - score: -6.8893\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 54.1580 - score: -6.8893\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 59.7566 - score: -6.9932\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 59.7566 - score: -6.9932\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.8103 - score: -6.8782\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 49.8103 - score: -6.8782\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 61.4907 - score: -6.9657\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 61.4907 - score: -6.9657\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 72.8040 - score: -7.1525\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "63/63 [==============================] - 7s 118ms/step - loss: 72.8040 - score: -7.1525\n",
      "epoch :  4\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.2536 - score: -6.9877\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 56.2536 - score: -6.9877\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 64.0440 - score: -6.9930\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 64.0440 - score: -6.9930\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.4419 - score: -6.9077\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 54.4419 - score: -6.9077\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 59.9866 - score: -6.9540\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 59.9866 - score: -6.9540\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 65.5376 - score: -7.0550\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 65.5376 - score: -7.0550\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 60.2000 - score: -6.9547\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 60.2000 - score: -6.9547\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 59.2840 - score: -7.0029\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 59.2840 - score: -7.0029\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.5971 - score: -6.8775\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 55.5971 - score: -6.8775\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.5312 - score: -6.8926\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 53.5312 - score: -6.8926\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 64.9356 - score: -7.0016\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 64.9356 - score: -7.0016\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.1778 - score: -6.8729\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 55.1778 - score: -6.8729\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 61.6054 - score: -7.0350\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 61.6054 - score: -7.0350\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.9912 - score: -6.9983\n",
      "Epoch 00001: loss did not improve from 48.62673\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 58.9912 - score: -6.9983\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.2683 - score: -6.8624\n",
      "Epoch 00001: loss improved from 48.62673 to 47.26832, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 379ms/step - loss: 47.2683 - score: -6.8624\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 62.1049 - score: -6.9899\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 62.1049 - score: -6.9899\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.2874 - score: -6.8437\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 54.2874 - score: -6.8437\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.8061 - score: -6.8900\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 53.8061 - score: -6.8900\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 62.1405 - score: -6.9884\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 62.1405 - score: -6.9884\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 60.6176 - score: -6.9776\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 60.6176 - score: -6.9776\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.8840 - score: -6.8146\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 48.8840 - score: -6.8146\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.5606 - score: -6.9470\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 58.5606 - score: -6.9470\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.4276 - score: -6.8449\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 49.4276 - score: -6.8449\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 61.4289 - score: -6.9490\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 61.4289 - score: -6.9490\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 67.8302 - score: -7.1050\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "63/63 [==============================] - 7s 118ms/step - loss: 67.8302 - score: -7.1050\n",
      "epoch :  5\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.9021 - score: -6.9767\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 54.9021 - score: -6.9767\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 59.3140 - score: -7.0109\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 59.3140 - score: -7.0109\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.1029 - score: -6.9801\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 57.1029 - score: -6.9801\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.6692 - score: -6.9020\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 56.6692 - score: -6.9020\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 61.5381 - score: -7.0029\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 61.5381 - score: -7.0029\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.4179 - score: -6.9189\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 58.4179 - score: -6.9189\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.7203 - score: -6.9570\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 57.7203 - score: -6.9570\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.5364 - score: -6.8325\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 53.5364 - score: -6.8325\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.9370 - score: -6.7674\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 48.9370 - score: -6.7674\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 59.5153 - score: -6.8960\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 59.5153 - score: -6.8960\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.0356 - score: -6.8530\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 54.0356 - score: -6.8530\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.0043 - score: -6.9683\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 58.0043 - score: -6.9683\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.6269 - score: -6.9384\n",
      "Epoch 00001: loss did not improve from 47.26832\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 56.6269 - score: -6.9384\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.2547 - score: -6.7864\n",
      "Epoch 00001: loss improved from 47.26832 to 45.25474, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 372ms/step - loss: 45.2547 - score: -6.7864\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.6129 - score: -6.9313\n",
      "Epoch 00001: loss did not improve from 45.25474\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 58.6129 - score: -6.9313\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.0107 - score: -6.8201\n",
      "Epoch 00001: loss did not improve from 45.25474\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 53.0107 - score: -6.8201\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.3394 - score: -6.9153\n",
      "Epoch 00001: loss did not improve from 45.25474\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 54.3394 - score: -6.9153\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 60.9401 - score: -6.9572\n",
      "Epoch 00001: loss did not improve from 45.25474\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 60.9401 - score: -6.9572\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.7295 - score: -6.9208\n",
      "Epoch 00001: loss did not improve from 45.25474\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 57.7295 - score: -6.9208\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.0320 - score: -6.8194\n",
      "Epoch 00001: loss did not improve from 45.25474\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 49.0320 - score: -6.8194\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 59.1907 - score: -6.9302\n",
      "Epoch 00001: loss did not improve from 45.25474\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 59.1907 - score: -6.9302\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.7901 - score: -6.7365\n",
      "Epoch 00001: loss improved from 45.25474 to 44.79011, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 373ms/step - loss: 44.7901 - score: -6.7365\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.3464 - score: -6.9127\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 57.3464 - score: -6.9127\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 67.0472 - score: -7.0684\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 67.0472 - score: -7.0684\n",
      "epoch :  6\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.2496 - score: -6.8667\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 50.2496 - score: -6.8667\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.6880 - score: -6.9438\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 57.6880 - score: -6.9438\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.7686 - score: -6.8991\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 54.7686 - score: -6.8991\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.7773 - score: -6.9045\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 55.7773 - score: -6.9045\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 60.1335 - score: -6.9874\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 60.1335 - score: -6.9874\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.8761 - score: -6.9211\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 57.8761 - score: -6.9211\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.7739 - score: -6.9599\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 58.7739 - score: -6.9599\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.2231 - score: -6.8187\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 52.2231 - score: -6.8187\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.8244 - score: -6.7465\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 47.8244 - score: -6.7465\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 59.3304 - score: -6.8995\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 59.3304 - score: -6.8995\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.4991 - score: -6.8463\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 52.4991 - score: -6.8463\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.8063 - score: -6.9664\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 55.8063 - score: -6.9664\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.8563 - score: -6.9313\n",
      "Epoch 00001: loss did not improve from 44.79011\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 57.8563 - score: -6.9313\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 40.8595 - score: -6.6953\n",
      "Epoch 00001: loss improved from 44.79011 to 40.85954, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 366ms/step - loss: 40.8595 - score: -6.6953\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.1683 - score: -6.8935\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 56.1683 - score: -6.8935\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.2756 - score: -6.7886\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 50.2756 - score: -6.7886\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.3429 - score: -6.8637\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 52.3429 - score: -6.8637\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.0208 - score: -6.9130\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 58.0208 - score: -6.9130\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.4710 - score: -6.9104\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 57.4710 - score: -6.9104\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.7342 - score: -6.6892\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 45.7342 - score: -6.6892\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.3772 - score: -6.8592\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 54.3772 - score: -6.8592\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 43.1424 - score: -6.6773\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 43.1424 - score: -6.6773\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.0799 - score: -6.8644\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 55.0799 - score: -6.8644\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 59.5824 - score: -6.9025\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 59.5824 - score: -6.9025\n",
      "epoch :  7\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.1728 - score: -6.9267\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 54.1728 - score: -6.9267\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 59.3637 - score: -6.9451\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 59.3637 - score: -6.9451\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.4863 - score: -6.8446\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 53.4863 - score: -6.8446\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.4880 - score: -6.8275\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 52.4880 - score: -6.8275\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.2450 - score: -6.9366\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 58.2450 - score: -6.9366\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.8377 - score: -6.9152\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 57.8377 - score: -6.9152\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.1451 - score: -6.9087\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 54.1451 - score: -6.9087\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.9202 - score: -6.8101\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 52.9202 - score: -6.8101\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.6291 - score: -6.6869\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 45.6291 - score: -6.6869\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 59.7863 - score: -6.8964\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 59.7863 - score: -6.8964\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.7265 - score: -6.8847\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 55.7265 - score: -6.8847\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.1712 - score: -6.9691\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 55.1712 - score: -6.9691\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.6597 - score: -6.8492\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 52.6597 - score: -6.8492\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 40.9771 - score: -6.6782\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 40.9771 - score: -6.6782\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.4457 - score: -6.8993\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 57.4457 - score: -6.8993\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.1324 - score: -6.7603\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 49.1324 - score: -6.7603\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.3585 - score: -6.7841\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 49.3585 - score: -6.7841\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.5972 - score: -6.9227\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 56.5972 - score: -6.9227\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.8141 - score: -6.8438\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 53.8141 - score: -6.8438\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 43.1159 - score: -6.6791\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 43.1159 - score: -6.6791\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.6020 - score: -6.7785\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 51.6020 - score: -6.7785\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.8925 - score: -6.7428\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 44.8925 - score: -6.7428\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.1375 - score: -6.8835\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 55.1375 - score: -6.8835\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 60.8355 - score: -6.8852\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 60.8355 - score: -6.8852\n",
      "epoch :  8\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.7014 - score: -6.8816\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 53.7014 - score: -6.8816\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.2216 - score: -6.8562\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 56.2216 - score: -6.8562\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.8560 - score: -6.8154\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 49.8560 - score: -6.8154\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.4741 - score: -6.7967\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 52.4741 - score: -6.7967\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.4889 - score: -6.9398\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 58.4889 - score: -6.9398\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.1246 - score: -6.9066\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 57.1246 - score: -6.9066\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.4265 - score: -6.8967\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 53.4265 - score: -6.8967\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.1915 - score: -6.7562\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 51.1915 - score: -6.7562\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.2243 - score: -6.6471\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 44.2243 - score: -6.6471\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.7154 - score: -6.8724\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 56.7154 - score: -6.8724\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.9848 - score: -6.8647\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 56.9848 - score: -6.8647\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.9932 - score: -6.9099\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 53.9932 - score: -6.9099\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.3651 - score: -6.8537\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 52.3651 - score: -6.8537\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 41.3776 - score: -6.6554\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 41.3776 - score: -6.6554\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.2618 - score: -6.8386\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 53.2618 - score: -6.8386\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.3490 - score: -6.6886\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 47.3490 - score: -6.6886\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.9066 - score: -6.7823\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 48.9066 - score: -6.7823\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.1019 - score: -6.8652\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 54.1019 - score: -6.8652\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.8269 - score: -6.8832\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 54.8269 - score: -6.8832\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.4642 - score: -6.6957\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 44.4642 - score: -6.6957\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.5349 - score: -6.7648\n",
      "Epoch 00001: loss did not improve from 40.85954\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 50.5349 - score: -6.7648\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 40.3290 - score: -6.6204\n",
      "Epoch 00001: loss improved from 40.85954 to 40.32898, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 403ms/step - loss: 40.3290 - score: -6.6204\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.1092 - score: -6.8126\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 52.1092 - score: -6.8126\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 55.0505 - score: -6.8563\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 55.0505 - score: -6.8563\n",
      "epoch :  9\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.0166 - score: -6.7160\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 45.0166 - score: -6.7160\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.7739 - score: -6.8232\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 52.7739 - score: -6.8232\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.6313 - score: -6.7912\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 50.6313 - score: -6.7912\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.9739 - score: -6.7743\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 51.9739 - score: -6.7743\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.5046 - score: -6.9236\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 57.5046 - score: -6.9236\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.2462 - score: -6.8766\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 55.2462 - score: -6.8766\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.2782 - score: -6.8620\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 51.2782 - score: -6.8620\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.0257 - score: -6.7893\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 52.0257 - score: -6.7893\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.3968 - score: -6.6794\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 44.3968 - score: -6.6794\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.1751 - score: -6.8126\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 56.1751 - score: -6.8126\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.9908 - score: -6.8717\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 57.9908 - score: -6.8717\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.1953 - score: -6.8722\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 49.1953 - score: -6.8722\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.2012 - score: -6.8422\n",
      "Epoch 00001: loss did not improve from 40.32898\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 53.2012 - score: -6.8422\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 38.1819 - score: -6.5959\n",
      "Epoch 00001: loss improved from 40.32898 to 38.18190, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 397ms/step - loss: 38.1819 - score: -6.5959\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.5991 - score: -6.7676\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 49.5991 - score: -6.7676\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.6685 - score: -6.6352\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 44.6685 - score: -6.6352\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.3754 - score: -6.7326\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 48.3754 - score: -6.7326\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.0917 - score: -6.8228\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 52.0917 - score: -6.8228\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.6353 - score: -6.8170\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 51.6353 - score: -6.8170\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 39.6233 - score: -6.5862\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 39.6233 - score: -6.5862\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.0596 - score: -6.7449\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 49.0596 - score: -6.7449\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 39.1323 - score: -6.6137\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 39.1323 - score: -6.6137\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.1090 - score: -6.7641\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 50.1090 - score: -6.7641\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 55.0730 - score: -6.7996\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 55.0730 - score: -6.7996\n",
      "epoch :  10\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.0976 - score: -6.7884\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 49.0976 - score: -6.7884\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.4111 - score: -6.7983\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 52.4111 - score: -6.7983\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.5254 - score: -6.7455\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 49.5254 - score: -6.7455\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.8991 - score: -6.8258\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 54.8991 - score: -6.8258\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.9262 - score: -6.9222\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 57.9262 - score: -6.9222\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 58.6202 - score: -6.9162\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 58.6202 - score: -6.9162\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.9487 - score: -6.8626\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 49.9487 - score: -6.8626\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.2048 - score: -6.7368\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 48.2048 - score: -6.7368\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 39.9202 - score: -6.6077\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 39.9202 - score: -6.6077\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 59.8125 - score: -6.8971\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 59.8125 - score: -6.8971\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.5932 - score: -6.8359\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 53.5932 - score: -6.8359\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.1074 - score: -6.8790\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 49.1074 - score: -6.8790\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.1249 - score: -6.8074\n",
      "Epoch 00001: loss did not improve from 38.18190\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 52.1249 - score: -6.8074\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 36.7628 - score: -6.5819\n",
      "Epoch 00001: loss improved from 38.18190 to 36.76279, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 379ms/step - loss: 36.7628 - score: -6.5819\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.3496 - score: -6.7910\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 51.3496 - score: -6.7910\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.5919 - score: -6.7088\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 47.5919 - score: -6.7088\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.1096 - score: -6.7845\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 49.1096 - score: -6.7845\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.3440 - score: -6.8149\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 51.3440 - score: -6.8149\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.0658 - score: -6.8362\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 52.0658 - score: -6.8362\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 38.6187 - score: -6.5354\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 38.6187 - score: -6.5354\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.9391 - score: -6.7442\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 49.9391 - score: -6.7442\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 39.4679 - score: -6.6058\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 39.4679 - score: -6.6058\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.7658 - score: -6.7557\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 49.7658 - score: -6.7557\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 55.4749 - score: -6.7817\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 55.4749 - score: -6.7817\n",
      "epoch :  11\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.0920 - score: -6.8272\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 53.0920 - score: -6.8272\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.4783 - score: -6.8916\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 57.4783 - score: -6.8916\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.7948 - score: -6.7050\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 48.7948 - score: -6.7050\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.7943 - score: -6.7854\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 55.7943 - score: -6.7854\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.1530 - score: -6.8690\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 54.1530 - score: -6.8690\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.1877 - score: -6.8903\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 56.1877 - score: -6.8903\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.1814 - score: -6.8511\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 51.1814 - score: -6.8511\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.6983 - score: -6.7684\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 50.6983 - score: -6.7684\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 40.1879 - score: -6.6151\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 40.1879 - score: -6.6151\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.7709 - score: -6.8517\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 56.7709 - score: -6.8517\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.4720 - score: -6.7789\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 52.4720 - score: -6.7789\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 46.4218 - score: -6.8131\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 46.4218 - score: -6.8131\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.6586 - score: -6.7652\n",
      "Epoch 00001: loss did not improve from 36.76279\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 50.6586 - score: -6.7652\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 36.4486 - score: -6.5878\n",
      "Epoch 00001: loss improved from 36.76279 to 36.44857, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 363ms/step - loss: 36.4486 - score: -6.5878\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.3193 - score: -6.8013\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 50.3193 - score: -6.8013\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.6700 - score: -6.6787\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 44.6700 - score: -6.6787\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.4211 - score: -6.7082\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 44.4211 - score: -6.7082\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.7375 - score: -6.8140\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 50.7375 - score: -6.8140\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.8189 - score: -6.8009\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 50.8189 - score: -6.8009\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 38.9339 - score: -6.5862\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 38.9339 - score: -6.5862\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.6388 - score: -6.7159\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 47.6388 - score: -6.7159\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 38.2766 - score: -6.5753\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 38.2766 - score: -6.5753\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.5668 - score: -6.7403\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 48.5668 - score: -6.7403\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 52.3036 - score: -6.7613\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 52.3036 - score: -6.7613\n",
      "epoch :  12\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 46.8819 - score: -6.7695\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 46.8819 - score: -6.7695\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.0302 - score: -6.8035\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 52.0302 - score: -6.8035\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.0487 - score: -6.7274\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 47.0487 - score: -6.7274\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.3454 - score: -6.7344\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 51.3454 - score: -6.7344\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.0592 - score: -6.8948\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 56.0592 - score: -6.8948\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.0986 - score: -6.8310\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 54.0986 - score: -6.8310\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.9138 - score: -6.8455\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 49.9138 - score: -6.8455\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.3247 - score: -6.7972\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 52.3247 - score: -6.7972\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 40.2943 - score: -6.5762\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 40.2943 - score: -6.5762\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.2388 - score: -6.8388\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 55.2388 - score: -6.8388\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.2794 - score: -6.7961\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 52.2794 - score: -6.7961\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.6916 - score: -6.8456\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 45.6916 - score: -6.8456\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.8566 - score: -6.8005\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 51.8566 - score: -6.8005\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 38.3702 - score: -6.5981\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 38.3702 - score: -6.5981\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.8759 - score: -6.7689\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 49.8759 - score: -6.7689\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.3009 - score: -6.6369\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 44.3009 - score: -6.6369\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.3186 - score: -6.7109\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 45.3186 - score: -6.7109\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.9653 - score: -6.8005\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 49.9653 - score: -6.8005\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.9331 - score: -6.7803\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 50.9331 - score: -6.7803\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 40.1233 - score: -6.5754\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 40.1233 - score: -6.5754\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.2764 - score: -6.6916\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 47.2764 - score: -6.6916\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 39.7088 - score: -6.6293\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 39.7088 - score: -6.6293\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 46.2932 - score: -6.6653\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 46.2932 - score: -6.6653\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 54.3487 - score: -6.7904\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "63/63 [==============================] - 7s 118ms/step - loss: 54.3487 - score: -6.7904\n",
      "epoch :  13\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.0083 - score: -6.7757\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 48.0083 - score: -6.7757\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.5127 - score: -6.7957\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 52.5127 - score: -6.7957\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 46.9494 - score: -6.7104\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 46.9494 - score: -6.7104\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.0267 - score: -6.7461\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 52.0267 - score: -6.7461\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.3109 - score: -6.8871\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 55.3109 - score: -6.8871\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.8954 - score: -6.8350\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 53.8954 - score: -6.8350\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.4685 - score: -6.8239\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 50.4685 - score: -6.8239\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.3107 - score: -6.7296\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 48.3107 - score: -6.7296\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 40.1284 - score: -6.5913\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 40.1284 - score: -6.5913\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.9317 - score: -6.8226\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 54.9317 - score: -6.8226\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.4629 - score: -6.8120\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 53.4629 - score: -6.8120\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 42.4825 - score: -6.7501\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 42.4825 - score: -6.7501\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.5120 - score: -6.7877\n",
      "Epoch 00001: loss did not improve from 36.44857\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 51.5120 - score: -6.7877\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 36.4393 - score: -6.5218\n",
      "Epoch 00001: loss improved from 36.44857 to 36.43932, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 377ms/step - loss: 36.4393 - score: -6.5218\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.3407 - score: -6.7049\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 47.3407 - score: -6.7049\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 43.3248 - score: -6.5997\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 43.3248 - score: -6.5997\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.5148 - score: -6.7703\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 48.5148 - score: -6.7703\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.0667 - score: -6.7215\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 47.0667 - score: -6.7215\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.3360 - score: -6.7576\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 50.3360 - score: -6.7576\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 37.1343 - score: -6.5406\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 37.1343 - score: -6.5406\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.2097 - score: -6.7103\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 48.2097 - score: -6.7103\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 37.7049 - score: -6.5773\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 37.7049 - score: -6.5773\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 46.4937 - score: -6.6851\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 46.4937 - score: -6.6851\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 47.3006 - score: -6.6492\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "63/63 [==============================] - 7s 116ms/step - loss: 47.3006 - score: -6.6492\n",
      "epoch :  14\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.0290 - score: -6.6626\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 44.0290 - score: -6.6626\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.1590 - score: -6.8210\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 55.1590 - score: -6.8210\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.8569 - score: -6.6690\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 44.8569 - score: -6.6690\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.8061 - score: -6.6522\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 186ms/step - loss: 48.8061 - score: -6.6522\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.5784 - score: -6.8459\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 55.5784 - score: -6.8459\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.7312 - score: -6.8034\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 52.7312 - score: -6.8034\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.7921 - score: -6.7808\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 47.7921 - score: -6.7808\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.4020 - score: -6.7848\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 52.4020 - score: -6.7848\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 40.7700 - score: -6.6375\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 40.7700 - score: -6.6375\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 57.5814 - score: -6.8433\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 57.5814 - score: -6.8433\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.0733 - score: -6.7645\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 53.0733 - score: -6.7645\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 41.2275 - score: -6.7286\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 41.2275 - score: -6.7286\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.8564 - score: -6.8057\n",
      "Epoch 00001: loss did not improve from 36.43932\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 51.8564 - score: -6.8057\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 36.2633 - score: -6.5665\n",
      "Epoch 00001: loss improved from 36.43932 to 36.26333, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 377ms/step - loss: 36.2633 - score: -6.5665\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.2915 - score: -6.7272\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 48.2915 - score: -6.7272\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 40.9551 - score: -6.5715\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 40.9551 - score: -6.5715\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.6303 - score: -6.7527\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 48.6303 - score: -6.7527\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.9999 - score: -6.7713\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 48.9999 - score: -6.7713\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.4728 - score: -6.7647\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 50.4728 - score: -6.7647\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 37.4333 - score: -6.5541\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 37.4333 - score: -6.5541\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.7056 - score: -6.7014\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 45.7056 - score: -6.7014\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 36.7091 - score: -6.5760\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 36.7091 - score: -6.5760\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 46.0128 - score: -6.6744\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 46.0128 - score: -6.6744\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 51.9839 - score: -6.7413\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 51.9839 - score: -6.7413\n",
      "epoch :  15\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.6615 - score: -6.6495\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 45.6615 - score: -6.6495\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.1797 - score: -6.7473\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 52.1797 - score: -6.7473\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 41.7489 - score: -6.5795\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 41.7489 - score: -6.5795\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.2890 - score: -6.7011\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 50.2890 - score: -6.7011\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.9992 - score: -6.8479\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 54.9992 - score: -6.8479\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.0705 - score: -6.8509\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 55.0705 - score: -6.8509\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.7528 - score: -6.8364\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 50.7528 - score: -6.8364\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.9597 - score: -6.7473\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 49.9597 - score: -6.7473\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 37.8514 - score: -6.5330\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 37.8514 - score: -6.5330\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.6314 - score: -6.7790\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 53.6314 - score: -6.7790\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.3534 - score: -6.7659\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 53.3534 - score: -6.7659\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 42.7464 - score: -6.7253\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 42.7464 - score: -6.7253\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.8173 - score: -6.7255\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 48.8173 - score: -6.7255\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 37.0066 - score: -6.5504\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 37.0066 - score: -6.5504\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.3882 - score: -6.7440\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 50.3882 - score: -6.7440\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.8099 - score: -6.6556\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 44.8099 - score: -6.6556\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.4641 - score: -6.7308\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 45.4641 - score: -6.7308\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.3792 - score: -6.7372\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 47.3792 - score: -6.7372\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.9070 - score: -6.8130\n",
      "Epoch 00001: loss did not improve from 36.26333\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 51.9070 - score: -6.8130\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 34.8924 - score: -6.4998\n",
      "Epoch 00001: loss improved from 36.26333 to 34.89241, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 361ms/step - loss: 34.8924 - score: -6.4998\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 46.9791 - score: -6.7248\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 46.9791 - score: -6.7248\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 39.5197 - score: -6.5888\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 39.5197 - score: -6.5888\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.6714 - score: -6.6714\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 45.6714 - score: -6.6714\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 48.6138 - score: -6.7025\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 48.6138 - score: -6.7025\n",
      "epoch :  16\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.8518 - score: -6.6808\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 44.8518 - score: -6.6808\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.5582 - score: -6.7565\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 50.5582 - score: -6.7565\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 42.2572 - score: -6.6288\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 42.2572 - score: -6.6288\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.8223 - score: -6.7248\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 50.8223 - score: -6.7248\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.5173 - score: -6.7932\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 51.5173 - score: -6.7932\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.1619 - score: -6.8101\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 52.1619 - score: -6.8101\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.9042 - score: -6.7849\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 48.9042 - score: -6.7849\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.7691 - score: -6.7144\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 48.7691 - score: -6.7144\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 36.9348 - score: -6.5424\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 36.9348 - score: -6.5424\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 54.5689 - score: -6.7823\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 54.5689 - score: -6.7823\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.3513 - score: -6.7636\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 53.3513 - score: -6.7636\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 39.4041 - score: -6.6577\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 39.4041 - score: -6.6577\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.6967 - score: -6.7310\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 191ms/step - loss: 48.6967 - score: -6.7310\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 35.9413 - score: -6.5464\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 35.9413 - score: -6.5464\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.6479 - score: -6.6721\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 45.6479 - score: -6.6721\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 42.7340 - score: -6.5729\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 42.7340 - score: -6.5729\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 43.9883 - score: -6.6735\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 43.9883 - score: -6.6735\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.5337 - score: -6.7887\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 48.5337 - score: -6.7887\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.0070 - score: -6.7065\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 47.0070 - score: -6.7065\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 38.3669 - score: -6.5447\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 38.3669 - score: -6.5447\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.3720 - score: -6.6769\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 45.3720 - score: -6.6769\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 36.3805 - score: -6.5607\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 36.3805 - score: -6.5607\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 43.7861 - score: -6.6136\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 43.7861 - score: -6.6136\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 50.8820 - score: -6.7760\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 50.8820 - score: -6.7760\n",
      "epoch :  17\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 40.1162 - score: -6.5973\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 40.1162 - score: -6.5973\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.2974 - score: -6.7504\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 51.2974 - score: -6.7504\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.0646 - score: -6.6585\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 44.0646 - score: -6.6585\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.2758 - score: -6.6692\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 48.2758 - score: -6.6692\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.8110 - score: -6.8030\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 51.8110 - score: -6.8030\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.8566 - score: -6.7964\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 51.8566 - score: -6.7964\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.1618 - score: -6.8215\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 48.1618 - score: -6.8215\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.8004 - score: -6.7341\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 48.8004 - score: -6.7341\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 37.6941 - score: -6.5759\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 37.6941 - score: -6.5759\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.4625 - score: -6.8263\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 56.4625 - score: -6.8263\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.2383 - score: -6.7973\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 56.2383 - score: -6.7973\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 40.5958 - score: -6.7019\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 40.5958 - score: -6.7019\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.6526 - score: -6.7213\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 48.6526 - score: -6.7213\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 34.9189 - score: -6.5393\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 34.9189 - score: -6.5393\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 48.1284 - score: -6.7291\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 48.1284 - score: -6.7291\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 43.1986 - score: -6.5711\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 43.1986 - score: -6.5711\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.8716 - score: -6.6875\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 45.8716 - score: -6.6875\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.3093 - score: -6.7391\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 47.3093 - score: -6.7391\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 46.2937 - score: -6.7035\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 46.2937 - score: -6.7035\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 37.0143 - score: -6.5271\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 37.0143 - score: -6.5271\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.0794 - score: -6.7150\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 49.0794 - score: -6.7150\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 35.8585 - score: -6.5470\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 35.8585 - score: -6.5470\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 43.7979 - score: -6.6255\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 43.7979 - score: -6.6255\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 46.2874 - score: -6.6341\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 46.2874 - score: -6.6341\n",
      "epoch :  18\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.2527 - score: -6.6673\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 44.2527 - score: -6.6673\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.4522 - score: -6.7246\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 50.4522 - score: -6.7246\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.6204 - score: -6.7002\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 45.6204 - score: -6.7002\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.8327 - score: -6.6934\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 49.8327 - score: -6.6934\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.6440 - score: -6.8035\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 51.6440 - score: -6.8035\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 53.2830 - score: -6.8151\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 53.2830 - score: -6.8151\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.5667 - score: -6.7773\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 47.5667 - score: -6.7773\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.9013 - score: -6.7435\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 49.9013 - score: -6.7435\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 38.2651 - score: -6.5512\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 38.2651 - score: -6.5512\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 55.7154 - score: -6.8418\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 55.7154 - score: -6.8418\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.5169 - score: -6.7490\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 51.5169 - score: -6.7490\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 39.1341 - score: -6.6866\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 39.1341 - score: -6.6866\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 50.8142 - score: -6.7761\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 50.8142 - score: -6.7761\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 36.3477 - score: -6.5411\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 36.3477 - score: -6.5411\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.2106 - score: -6.7224\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 47.2106 - score: -6.7224\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 42.8435 - score: -6.6005\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 42.8435 - score: -6.6005\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.7190 - score: -6.6766\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 44.7190 - score: -6.6766\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.0825 - score: -6.7044\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 45.0825 - score: -6.7044\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.4431 - score: -6.7480\n",
      "Epoch 00001: loss did not improve from 34.89241\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 49.4431 - score: -6.7480\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 34.8507 - score: -6.4859\n",
      "Epoch 00001: loss improved from 34.89241 to 34.85067, saving model to hopefully_model.hope\n",
      "16/16 [==============================] - 6s 387ms/step - loss: 34.8507 - score: -6.4859\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 44.1991 - score: -6.6381\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 44.1991 - score: -6.6381\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 38.4660 - score: -6.5728\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 38.4660 - score: -6.5728\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 43.3911 - score: -6.6226\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 43.3911 - score: -6.6226\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 46.7857 - score: -6.5866\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "63/63 [==============================] - 7s 119ms/step - loss: 46.7857 - score: -6.5866\n",
      "epoch :  19\n",
      "batch 0 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 43.2675 - score: -6.6336\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 43.2675 - score: -6.6336\n",
      "batch 1 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.0920 - score: -6.6765\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 49.0920 - score: -6.6765\n",
      "batch 2 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 42.9389 - score: -6.6048\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 42.9389 - score: -6.6048\n",
      "batch 3 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.9763 - score: -6.6601\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 47.9763 - score: -6.6601\n",
      "batch 4 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.0552 - score: -6.7207\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 49.0552 - score: -6.7207\n",
      "batch 5 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 52.1654 - score: -6.7960\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 52.1654 - score: -6.7960\n",
      "batch 6 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 46.6536 - score: -6.7760\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 46.6536 - score: -6.7760\n",
      "batch 7 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.2462 - score: -6.6961\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 47.2462 - score: -6.6961\n",
      "batch 8 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 36.5710 - score: -6.4822\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 36.5710 - score: -6.4822\n",
      "batch 9 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 56.0469 - score: -6.8184\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 56.0469 - score: -6.8184\n",
      "batch 10 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 51.7229 - score: -6.7511\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 51.7229 - score: -6.7511\n",
      "batch 11 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 39.4687 - score: -6.6846\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 39.4687 - score: -6.6846\n",
      "batch 12 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 49.7975 - score: -6.7240\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 49.7975 - score: -6.7240\n",
      "batch 13 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 36.1461 - score: -6.5719\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 36.1461 - score: -6.5719\n",
      "batch 14 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 46.0467 - score: -6.6987\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 46.0467 - score: -6.6987\n",
      "batch 15 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 38.9381 - score: -6.4959\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 38.9381 - score: -6.4959\n",
      "batch 16 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 45.6827 - score: -6.7249\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 45.6827 - score: -6.7249\n",
      "batch 17 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 46.1300 - score: -6.7154\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 46.1300 - score: -6.7154\n",
      "batch 18 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.4544 - score: -6.7170\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 47.4544 - score: -6.7170\n",
      "batch 19 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 37.1370 - score: -6.5381\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 37.1370 - score: -6.5381\n",
      "batch 20 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 47.6800 - score: -6.7332\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 47.6800 - score: -6.7332\n",
      "batch 21 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 37.3166 - score: -6.5772\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 37.3166 - score: -6.5772\n",
      "batch 22 outta 23\n",
      "16/16 [==============================] - ETA: 0s - loss: 40.9322 - score: -6.5955\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 40.9322 - score: -6.5955\n",
      "last batch\n",
      "63/63 [==============================] - ETA: 0s - loss: 48.7274 - score: -6.6491\n",
      "Epoch 00001: loss did not improve from 34.85067\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 48.7274 - score: -6.6491\n"
     ]
    }
   ],
   "source": [
    "training(data_ct,tr,batch_size=64,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:56:57.948711Z",
     "iopub.status.busy": "2020-09-27T22:56:57.947707Z",
     "iopub.status.idle": "2020-09-27T22:56:58.088519Z",
     "shell.execute_reply": "2020-09-27T22:56:58.087956Z"
    },
    "papermill": {
     "duration": 2.965245,
     "end_time": "2020-09-27T22:56:58.088661",
     "exception": false,
     "start_time": "2020-09-27T22:56:55.123416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(\"/kaggle/working/after_all_epochs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:57:04.732147Z",
     "iopub.status.busy": "2020-09-27T22:57:04.730309Z",
     "iopub.status.idle": "2020-09-27T22:57:04.732761Z",
     "shell.execute_reply": "2020-09-27T22:57:04.733166Z"
    },
    "papermill": {
     "duration": 3.123555,
     "end_time": "2020-09-27T22:57:04.733276",
     "exception": false,
     "start_time": "2020-09-27T22:57:01.609721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/gender_smkstat_cols_norm.var\",\"wb\") as f:\n",
    "    pickle.dump([gender,smkstat,cols_norm],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:57:10.229727Z",
     "iopub.status.busy": "2020-09-27T22:57:10.228145Z",
     "iopub.status.idle": "2020-09-27T22:57:10.230879Z",
     "shell.execute_reply": "2020-09-27T22:57:10.231454Z"
    },
    "papermill": {
     "duration": 2.804711,
     "end_time": "2020-09-27T22:57:10.231603",
     "exception": false,
     "start_time": "2020-09-27T22:57:07.426892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FE=[\"Percent_norm\",\"Age_norm\",\"min_FVC_norm\",\"base_week_norm\",'gender','Currently smokes','Ex-smoker','Never smoked']\n",
    "def test(CTs,Tabular,batch_size=1):\n",
    "    TabularLentgh=len(Tabular)\n",
    "    Number_of_Batchs=int(TabularLentgh/batch_size)\n",
    "    for i in range(Number_of_Batchs):\n",
    "        cts , tabular = MiniBatch(CTs,Tabular,i,batch_size)\n",
    "        Y=tabular.FVC\n",
    "        X=tabular[FE]\n",
    "        X=X.values\n",
    "        X=tf.constant(X, dtype=tf.float64)\n",
    "        Y=tf.constant(Y.values, dtype=tf.float64)\n",
    "        cts=cts.reshape(batch_size,32,256,256,1)\n",
    "        preds=model.predict(x=[X,cts],batch_size=1)\n",
    "        if i==0:\n",
    "            fullpred=preds\n",
    "        else:\n",
    "            fullpred=np.concatenate((fullpred,preds))\n",
    "    if(TabularLentgh%batch_size != 0  ):\n",
    "        cts , tabular = MiniBatch(CTs,Tabular,Number_of_Batchs,TabularLentgh-batch_size*Number_of_Batchs)\n",
    "        Y=tabular.FVC\n",
    "        X=tabular[FE]\n",
    "        X=X.values\n",
    "        X=tf.constant(X, dtype=tf.float64)\n",
    "        Y=tf.constant(Y.values, dtype=tf.float64)\n",
    "        cts=cts.reshape(TabularLentgh-batch_size*Number_of_Batchs,32,256,256,1)\n",
    "        preds=model.predict(x=[X,cts],batch_size=1)\n",
    "        fullpred=np.concatenate((fullpred,preds))\n",
    "\n",
    "            \n",
    "    return fullpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T22:57:15.886621Z",
     "iopub.status.busy": "2020-09-27T22:57:15.885301Z",
     "iopub.status.idle": "2020-09-27T23:06:48.943787Z",
     "shell.execute_reply": "2020-09-27T23:06:48.942988Z"
    },
    "papermill": {
     "duration": 575.776112,
     "end_time": "2020-09-27T23:06:48.943931",
     "exception": false,
     "start_time": "2020-09-27T22:57:13.167819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16228.33948366246 7878.596\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydd3hVVfaw351eCR2kN0mEAEpTpPfeQRBUUBB0HB27wziObcY2lrH7iSU0KSI1Su+914QQkkA66b3dtr4/7s0llySkAKI/9vs898nNOrusc3Jy1t5rrbO3EhE0Go1Go3G61QpoNBqN5o+BNggajUajAbRB0Gg0Go0NbRA0Go1GA2iDoNFoNBobLrdagepSt25dadGixa1WQ6PRaP5UHDt2LFVE6pV17E9rEFq0aMHRo0dvtRoajUbzp0IpFV3eMe0y0mg0Gg2gDYJGo9FobGiDoNFoNBpAGwSNRqPR2NAGQaPRaDSANggajUajsaENgkaj0WgAbRA0GjsWsfBL+C8UmgpvtSoazS1BGwSNxsbBxIO8ceANNl7aeKtV0WhuCdogaDQ2jicdB+B8+vlbrIlGc2uo0CAopZoqpXYopc4ppUKUUn+zyWsrpbYopS7YftYqUWeeUipCKXVeKTW0hLyLUuqM7dhnSillk7srpZbb5IeUUi1u/KlqNNfmWNIxAMLSw26xJhrNraEyMwQT8IKI3AXcBzyllGoH/B3YJiJ3Attsv2M7NhVoDwwDvlJKOdva+hqYA9xp+wyzyWcBGSLSBvgEeP8GnJtGU2kMZgNnUs8A1hmC3lpWcztSoUEQkUQROW77ngOcAxoDY4EFtmILgHG272OBZSJSJCIXgQigu1LqDqCGiBwQ63/bwqvqFLe1EhhYPHvQaH4PQtNCKTIXcX+j+8kx5pCYl3irVdJofneqFEOwuXLuAQ4BDUQkEaxGA6hvK9YYiC1RLc4ma2z7frXcoY6ImIAsoE4Z/c9RSh1VSh1NSUmpiuoazTUpdhc9GPAgoN1GmtuTShsEpZQP8AvwrIhkX6toGTK5hvxadRwFIt+KSFcR6VqvXpnLeWs01eJ48nFa1GhB94bdUSgdWNbcllTKICilXLEagyUissomTrK5gbD9TLbJ44CmJao3ARJs8iZlyB3qKKVcAD8gvaono9FUB7PFzImkE3Rp0AUvVy+a12h+S2cIOYYcDiQcqHYcI7UgFZPFdIO10pSHwWzAYDbcajVuCJXJMlLA98A5Efm4xKF1wAzb9xnA2hLyqbbMoZZYg8eHbW6lHKXUfbY2H7mqTnFbk4Dt8jtE9SxiISw9jNC0UM6nnycl//+WGyrPmIdFLLdajT88EZkR5Bhz6NKgCwABtQM4n3FrZghHLh9h4rqJzNkyh/0J+6tcP9+Yz+jVo/n42McVF/6dyCrKYvDKwQz6eRAP//Ywf9/zdy5lXbrudvfG7+Wz45/dEP0e2/QYZ1PPVrmuiPD45sd5Zscz163HH4HKzBB6Ag8DA5RSJ22fEcB7wGCl1AVgsO13RCQEWAGEAhuBp0TEbGvrSeA7rIHmSGCDTf49UEcpFQE8jy1j6WazPnI9k9dPZkrwFCatn8SQX4bYc9H/7GQUZjDo50FMWj+JbdHb/rRZM3O3zOWjox/d1D6K4wedG3QGwL+2P/G58WQbruUZvbEYzUY+OfYJszbNws3Zjbqedfkx5Mcqt3P48mFyjbmsOL+iWgMcEbnhg4jtMdu5nHeZjvU64ursyo6YHfxr/7+u+55cFLqI+Wfmk1mYeV3t/HbxN45cPsKXJ7+sct2jSUc5nnycffH7iM4udyOyPw2VyTLaKyJKRDqKyN22z28ikiYiA0XkTtvP9BJ1/iMirUXEX0Q2lJAfFZFA27G/Fs8CRKRQRCaLSBsR6S4iUTfndB05lHiI2h61+az/Z3zc72NqudfisxOf/WkfniXZeGkjucZc8o35PLvzWaYET+FCxoVbrVaViMqMYn/CfpacW0JqQepN6+d48nEaeDWgkXcjAPxr+QMQnh5+0/q8msXnFvPD2R+Y2HYiK0at4JF2j3Ao8RAhaSFVamdv/F7cnd0xWUwEhQRVup7RbGT1hdWMWTOGCWsn3NDlO7bFbKORdyM+6vsRPwz9gRe6vsCJ5BPsittV7TZNFhMnk08CVwx6dVkTsQaFYm/83ir/jwSFBOHn7oezcuaXC79clx5/BG7rN5WPJx+nc/3O9G/Wn8HNBzO7w2yOJR3jYOLBW63adRMcGUzbWm0JHh/Mv3v+m8S8RN49/O7vrkehqZAFIQvIN+ZXue6GSxtQKEwWE8vCll2XHtmGbPsDpCQiwrGkY3Ru0JniTOeA2gEAv6vbaOOljXSs25HXe7yOl6sXk9pOwsfVh6CzQfYyBrOBTZc2UWAqKLMNEWFv/F56NOrByFYjWXF+BWkFaRX2vSt2F8NWDeNf+/+Fk3IiMiuSb09/W+VzSMpLYu6WucTnxttlecY89ifsZ2DzgfbrO/7O8TSv0ZxPj3+K2WIurzk7S8OWOlwHsL4rkm+y3lNHko5UWddiwjPCCU0LZW6nuXi6eLIwdGG5ZQ8nHnZwdUVmRrI7bjfT75pO3yZ9WRuxFqPZWG1d/gjctgbhct5l4nPjuaf+PXbZpLaTaOjdkC9OfPGnniVEZ0dzOvU0o1qNwsXJhbFtxjIrcBZHLh+p8ogTrNeqrIdpZQgKCeLDox+yIHRBxYVLICJsvLiRbg270b9pf5adX1bug7DAVFCu26DIXMSCkAUM/2U4D294mHWR6xyOR2VFkVqQStcGXe2yup51qe1R+3cLLCfkJhCaFsqg5oPsMl83Xyb7T2Zz9GZic2LJNmTzxNYneHHXizy17SnyjHml2rmYfZH43Hh6N+7N7A6zMVgMFV53EeH9I+/j5eLFN4O+Yc3YNYxuNZofQ34kKrNqE/UNFzewP2E/80/Pt8v2xO3BaDEyqNmVc3N1cuWv9/yViMwIfr346zXbtIiFb059w9envnYI3B5NOgpAm5ptOHr5aJX0LMmaiDW4OLkwLWAa49qMIzgquExXW44hhye2PsG036bZ/4eCQoLwcPZgqv9UJradSHphOjvjdlZblz8Ct61BKH7AFQcSAdyc3ZjbcS6nU0+zJ37PrVLtugmOCkahGNFyhF02se1EvF29WRBStQczwH8O/YfHNz9OjiGnSvXSC9MJCglCofjp3E8Os4TUglQeWP8AX538qsxRYlh6GJeyLzGs5TBmBs4kqyiLtRFrS5ULSQ1h3JpxTAmeUiqzJiQ1hFGrR/Hh0Q/pUK8Dnet35q0Db3Eu7Zxdh2d3PIu3qze9G/e211NK4V/L/3dLPd0avRXA4aEJ8NBdD+GknPj0+KfM2DCDE8kneDDgQY4nHWfulrml/h574/YC0KtxL1r6tWRYi2EsC1tGRmFGuX2HZ4QTmxPLI+0foWfjniileKHrC3i5ePH2wberNDAqdgGtjVzL5bzL1nOL2Uodjzp0qtfJoeyQ5kNoV6cdX5748poZOqFpoaQXppNvyudQ4iG7/HjScZr6NmVYi2GEZ4STVZRVaT2LMVqM/Br1K/2a9KOWRy0evuthLGLhp7CfSpXdHrMdo8WIs3Lm8c2PszN2J8FRwYy/czy1PGrRs1FPGng14JfwP7fb6LY1CMeSjuHp4ol/bX8H+dg2Y2ni0+S6ZgkXMi6QXnjjs2azirIqTCcUEYIjg+l+R3caeDewy33dfJl450Q2X9pMYm7l38LNN+azP34/heZCNl3aVG45s8VMWHqYwzWbf3o+BaYC3rj/DTKLMll1YZX92H+P/Jdz6ef4+tTXPLX9qVIj/A2XNuCiXBjcbDB317ubjnU7sjB0od14iAg/h//MwxseJsuQRUJeQqmsnM9PfI7BbOC7Id/xzaBv+Ljfx9R0r8lzO58jOjuaOVvmkJSfxFcDv+IOnzsc6gbUDiAiMwKjpXougIUhC5kaPJVxa8Yx7JdhvLL7lXLvp20x2/Cv5U/TGk0d5PW96jO61Wg2XdrE5bzLfDPoG/5x7z/4sO+HhKSFMHvzbIcH4d74vbTya0UjH2ssZE7HORSaCnl046N8ceILTqWcKhUw3hK9BSflxICmA+yyOp51eL7L8xxNOsrayNJG2GA2lJoxZhVlcSL5BKNajQKxjp6LzEXsjtvNgGYDcHZydijvpJx4tvOzJOQl8HP4z+Vexz1xe1AoPF082R67HbDOGordvd0adkMQ+4yhJCJCREYE++PLztbaE7eH9MJ0xrWxLpjQtEZTBjYbyPLzy0u5ODdc2kBjn8YsG7WMGm41eHr701jEwiPtHgHA2cmZ8XeOZ3/CfgeX2Z+N29YgnEg+Qad6nXBxcnGQuzq58kSnJziXfo7tMdvLrGu0GFlybgl9l/dl2q/TWBe5jiJzESGpIfxl61+YsG4CL+16qVS9Nw+8WaY8KS+pwhTDLdFb6LWsF10Wd2HAigHM3DiTi1kXS5U7lXKKuNw4RrcaXerYQ3c9BFgDmJVlb/xeDBYDXi5eZY7Qi/nh7A9MXj+ZNw68gcFsID43nuXnlzOuzTgm3DmBzvU7ExQShNFsZH/8fn67+BtPdnqS1+57jcOJh5kSPMU+FS92F/Vo1IOaHjVRSjGj/Qxic2L58uSXfHz0Y2ZsnMFbB96ie8PurB+3nlrutVgTscauT2xOLPsS9jHVfyr33nEvYH3QfdLvE5Lzkxm3dhzRWdF82v9Te3aRGAykfPY55qws/Gv7Y7QYy7zGFZFVlMXnJz4n35RPq5qtaF6jOb9d/I0dsTtKlU0tSOVE8gkGNh9YZltzO81lSPMhLBi+wH4eg5oP4tP+nxKeEc68PfOwiIV8Yz5Hk446zHRa12zN2z3fpoZ7Deafmc9Dvz3E6/tfd2h/a/RWOtfvTB1Px4UBxt85nnvq38NHRz8qNfr+5NgnPLzhYU6nnLbL9ifsxyxmpgZMZWSrkfwS/gvBkcEUmApKzXyK6dGoB53qdWJZ2LJyjeWe+D10rNeR3o17szN2JxaxcDHrIplFmXRp0IXAuoF4OHs4uI3ic+N5edfL9FvRj/HrxjN369wy44JrItZQx6MOPRv3tMtmtp9JjiGHFedX2GWZhZkcSjjE0BZDaezTmB+G/kCLGi0Y12YcTXyvvFo1vs14gD/1LOG2NAjZhmzCM8LpXL9zmcdHthpJM99mfHP6m1I36pHLR3hg/QO8d/g92tRsQ64xl1f3vkq/5f2Y+utUTqeepmfjnhy+fJgzKWfs9SIyIlgZvpKNlzYSmRnp0Oare19l7pa55a7DLyJ8f+Z7mvg04fEOj9O7SW+iMqN4cuuTpbJvgqOC8XD2cPBHF3OHzx0MaTGEXy78Umn3z9aYrdT2qM3jHR/nZMrJMh+QecY8FoQuoKF3Q1ZdWMVjmx7j/cPv46SceLLTkwDM7jCbpPwkVl1YxdsH36ZFjRbM7jCbB/wfYOHwhViwMGPDDIKjgjmVcorEvESGtxxu72Ngs4E09W3K/DPzWXxuMUazkee6PMeXA7+knlc9RrYayc7YnfaZxsrwlTgrZybcOcFB1w71OvDafa/h4ezBx/0+pkejHlfO49AhUr/6ipytW7mrzl0A/GPPP1gYspCkvKRKXS+wPmgKzYV80OcDPu73MV8O/JJWfq34+NjHpYKO26K3IQiDmw0us63GPo35qN9HtK3V1kHep0kfXun2Cnvi9/D9me85fPkwRouRXk16OZQb22YsC4cvZPeU3UxuO5k1EWvsrrCorCgisyLLvFeclBOv3vsq2YZsvjr5lV0enR1tD/CXDMDujN1JbY/aBNYJZFaHWRSZi3jv8Hv4uvnS7Y5u5V6rKf5TuJR9iSOXSweG0wrSOJt6lt6NezOg2QBSC1I5k3rGnlXUpUEX3Jzd6FSvk32GICK8uvdVdsXt4v5G9/Pm/W9Sy70WS84tcWg7tSCVPXF7GN16tMOgsGO9jvRs1JP5Z+bbDeHWmK2YxMSwFta1OBv5NGLtuLW83sPRuDbyaUTfpn2Zf2Y+r+17rcqZcWdTzzL8l+F8dvyz606lrS63pUE4lXwKQewjw6txcXJhbqe5hKWHOYzqDiUeYtamWRSYCvhf///x3ZDvWDt2LfOHzGdAswE8dfdTbJywkY/6foSvm69DHvn8M/PxdPHEzcmNpWFL7fLQtFAOXT6Er6sv/9z7T0JSSwd9T6WcIiQthJntZ/LXe/7Km/e/yZcDvyStII1ntj9jD7aeSTnDxksb6d+sP96u3mWe24z2M8gz5jmMgMrDYDawO243/Zr2Y2zrsTgr51JBWYBlYcvIKsri474f82HfDwnPCGdH7A6m3TWNht4NAatfO6B2AO8cfoe43Dheu+813JzdAAisG8iykcvoULcD8/bMY96eebg7u9O/aX97H85Ozvww9AeWjlzKwWkHWTpqKY8FPmZ3RYxrM87qE774K0azkTURa+jbpK+D26yY8XeOZ+/UvfRt2tdBXnjW+mJSUWQUrfxa8XqP13FSTvz36H8ZvHIwr+17zeEf1Wwxszd+r4OLwCIWloUto3P9zvZsJRcnF17s+qL1YXreMVtqa8xWWtRoQeuarSv8e1zNFP8pDG85nC9OfsH809b7q7xBjp+7H3/r/Dd8XX354uQX1r7LiV0U41/bn8ltJ7P8/HJ7OuYnxz7B1dmVMa3HsCV6C/G58ZgsJvbG76VX4144OznT0q8lg5sPptBcSP+m/XF1ci33HAY3H4yfux8rwkvfj/sS9iEIvZv0pneT3rgoF7bHbOdY0jHqetalqa/Vxda1YVfOp58nqyiL3y7+xrGkY7zY7UXe7f0uE+6cwKS2k9gVu4vYnCtLrH1z6hsEYVLbSaX6fa7Lc+QYcvj+7PeANQOseY3m9r8nWA2mkyr9+Hyv93s82v5RgqOCrfGrIx+y4vwKdsbuZEfMDj47/hmPb36cRzc+Wiq1NygkiKT8JL478x1DfxnKO4fe4e0DbzN782xGrx7Nq3tfZUfMjpu6o99taRBOJJ/AWTnToW6HcsuMaDnCOks4ZZ0lZBRmMG/PPFr4tWDVmFUMbGZNo1NKcd8d9/GfXv/hiU5P4OPmg7erN1P9p7I1eivR2dFEZ0ez8dJGpgZMZXjL4ayLXGcfoQedDcLb1Ztlo5ZRx6MOz2x/huT8ZAddFoUuwtfNl9Gtr7iBOtTrwPt93uds6lme3v40MzbMYNpv07BYrvg1y6J9nfb0adKH/3f6/1Xo6zyUeIg8Yx4Dmw2knlc9ejbuybrIdQ5B4HxjPgtDF3J/o/vpUK8DQ1sMZdHwRUwLmMbsDrPt5ZRSzAqchUUsjGk9hu53dHfoq45nHb4d8i3TAqYRlxtHnyZ98HHzcSjT0LshgXUD7YakJP61/bmr9l2sjVjLtphtpBem84D/A+We29U+bYCCM1aDYIi0zuAmtZ3EitErWDduHQ+3e5j1kesZu3YsayPWsuTcEkatHsWTW59k1qZZ9tHk3vi9xOXG2RfJK6Z3k970bNSTb059Yy+bWZjJkctHGNR8ENVZ3FcpxRs93qBFjRacTj3NvXfcW+a1KcbP3Y8Z7WewM3Ynp1NOszV6Kx3rdSzTaBbz17v/irerN+8feZ+jl4+yLWYbswJn8fQ9T+OEEz+d+4lTKafINmTTt8kVAzun4xzcnd2tMYVr4OHiwdjWY9kWva3UiHpP3B7qetYloHYANdxq0K1hN7bHbOd48nG6NOhiv2bFcYTdcbv56OhHtK/TngltrswMp/hPwVk522c2kZmRrAxfyQP+D9C8RvNSOvnX9md069EsCV1CSGoIRy4fYWiLoZX6G3m7evN81+dZPWY1XRt0ZdG5Rbx98G2e3v40z+x4hh/O/kBKfgpHk446DK7SCtLYFrONKf5TWDVmFX2b9mX5+eVsjt5MgbGAFjVasCN2B8/seIY+y/uw+sLqCnWpFiLyp/x06dJFqssjvz0iU9dPrbDcmgtrJDAoULZFb5Ontj4l9yy8R8LSwirVR0p+inRe2Fne2P+GvLrnVem6qKuk5KfI2dSzEhgUKItCFklcTpx0WtBJPjzyoYiIhKWFSbfF3eSB9Q9IRkGGiIjE58RLxwUd5aOjH5XZz+LQxRIYFChDVw6VRSGLJNeQW6Fu8Tnx0n1xd5mzeY5YLBa7PKcoR3KKcuy/v77vdbl3yb1SZCoSEZFNFzdJYFCg7I3bay+z4OwCCQwKlGOXj1XYr9lils2XNleo46GEQ5KSn1Jhe1ezJHSJBAYFyrCVw2ToyqFitpirVD+8V28J9Q+QC4MGl3k8LC1MpgVPk8CgQAkMCpSHfn1Igs4Gyd0L75a5W+aKyWySJ7Y8If2X9xeDyVC6/fRw6bigo8zdMlfeO/SezN40WwKDAuVsytkqn2tJIjIipOfSnvJr5K8Vls015Ervpb1l4tqJEhgUKD+e+bHCOsXXtc+yPjJgxQDJN+aLiMjLu16We5fcK2/uf1PuXnC3w70jImIwl74GZRGVGSWBQYEy//R8u8xoNkqPn3rIP/f+0y776dxP9mu/JHSJXV5kKpIui7pIjyU9JDAoUE4lnyrVx0s7X5IeS3pIniFPntzypPRY0kPSC9LL1SkhJ0E6L+wsfZb1kcCgQAlPD6/UuVyN0WyUy7mX5XTyaTmRdELyjflisVjkweAHZdjKYWI0G0VE5Psz30tgUKBEZkTa6159/Qxmg+yL3ydv7X+rzHOsLMBRKee5etvNEAxmA2dTz5brLirJyFYjaerb1O6TfKHrC6WyksqjrmddxrQZw9qItQRHBTOp7STqetalfZ32dKrXiaVhS1kQsgCFYvpd0wHryOTDvh8SkRHBQxseIjY7lmVhy1AoHvR/sMx+pt81nfXj1hM8PpiH2j1UrquoJI18GvFsl2fZn7DfnkWyM3YnI1ePZPSa0RxLOobZYmZH7A76NO5jH3X2a9oPP3c/loYtJSozipT8FIJCgujWsFulrqeTcmJw88EV6tj9ju7U9axbYXtXM7LVSFydXInLjWNS20llTunLw5iUhCklBec6dTDGxWEpLD0t96/tz8LhC/lfv/+xeMRiFo1YxIz2M5jXfR774vfxr/3/Ym/8Xia3nYyrc2k3yZ217uTBgAfZF7+P1RGricuJY2iLobSr067K51qS1jVbs3vKbka0GlFhWW9Xb2Z1mGV/6a6s+MHVPOD/AG1qtiG9MJ2/df4bni6eADzS/hHyjHn8HP4zXRp2KTWju5arqCQt/VrSvWF3VoavtGdBnU45TY4hxyFIXtKFeHW6eKd6ncgx5jC+zXg61utYqo9pd00jx5jDa/teY0/8HuZ0nEMtj1qlyhVzh88dTL9rOumF6bT2a82dte6s1LlcjYuTCw28G9ChXgfurn83ni6e9tlyXG4cW6O3YhELK8NX0qVBF1rVbGWve/X1c3Vy5f5G9/Naj9fKPMcbwW1nEELSQjBYDOX6Wkvi4uTCnI5zyDXm0rdJX6YFTKtSXzPbz8RkMeGknJjZfqZdPi1gGjE5MSwLW8aIViPsfnawBgu/G/odWUVZTP9tOj+H/8yg5oNKpUWWpIVfi1LZUhUxxX8Knet35oMjH/Davtd4evvT1POsZ31gbJrFmwfeJL0w3SH7xc3ZjdGtRrMrbhdj145lwM8DSClIYW7HuVXq+2bh5+7HgGYDcHFysacSVpbCM9YEAL9Ro0AEw6VLZZZzdnJmYPOBDnn1k9tOZmzrsayLXIeLcinTL13MK91e4fhDxzk47SAbJm7gw74fVstddDVVMX5T/KdQ37M+7eq0c8iSKQ8XJxfe7/M+T939lIMLqH2d9vYX+kq6i6rDZP/JxOfGszRsKadSThEcFYyzcnYI+jf0bkj7Ou3xdfWlTc02DvX7N+1PHY86/K3z38psv1O9TgTWCWRz9GYa+zRm2l0V/y/P6jCL+l71SyUm3Aj6N+tPixot+OHsDxxKPERsTuw175vfjfKmDn/0T3VdRt+d/k4CgwIlrSCtUuVNZpOsj1wvWUVZ1erv02OfyvdnvneQGUwG6be8nwQGBcr59PNl1ruUdUlG/DJCAoMC5UTSiWr1XREXMy9K54WdpUNQB/no6EdSZCqS7KJseXrb0xIYFCidF3aWPEOeQ50CY4EcSDggv0b+KotDF8uq8FUObqdbTUp+SrWm00kffyKh7QMl//RpCfUPkMzg4CrVLzAWyGMbH5N3D71b5b5vBZeyLklsdux1t3Mg4YD0XNpT4nPir6sdg8kg/Zf3t7uEAoMC5dGNj5YqdzL5pGyN3lp2GxW4qIIjgyUwKFA2XtxYab2q6nasCqvCV0lgUKAM/2W49FraSwpNhTetr5JwDZeRkj/pEg1du3aVo0er/sp6Qm4Cp1NOM6zlsIoL30R+jfqVqKwonr7n6XLLZBVlEZoW6jBKutEcuXwEd2d3hymoRSwsDVuKQlVqJPV/gZjHZmHKyKDFsqWcv6czdZ94gnrPlP+3KQ8RuSEj/tuRpLwkYnJiKDQVUmQuIrBuoMPs+XoRESIzI2lTq03FhX8HDGYDw38ZTnJBMo+0e4SXupV+R+lmoJQ6JiJdyzpWNT/D/wEa+TSyv8l5KxnZamSFZfzc/W6qMQBrhsbVOCkne1zjdkBEKAgJocaQITi5u+PapAlFUdVbcPdGGAMxmTAlJeHauHHFhcsg//gJ3Fu3wtnP77p1+T1p4N3gmhlP14tS6qYYA+Ply6CccG1Qv+LCJXBzduPRwEf56OhHfwx3EbdhDKEkYrFQFHURS0HZi6Zpbg+MsbFYsrLw6BAIgHurVvbU098bU0oKMY8+RsTgIRSGVX1xPXNODtGPPELSO7//yrblISYTSe+9T9xzzxH7178S+5enKDh16rrbzd60mctvvX3d7VgMBi7/+z8Y4qq+5ISIEDNrNnHVmE2CNSlkw8QNtPRrWa36N5rb2iBk//orUSNGcL5zFyIGDyH+pZcx5+bearVuCCJC+sKF5FfDrfZHIuaxWSR98N+b2keBLaDsGWg1CG6tW2G4dAkx/b7bUOYdPkzUhAkUnDmDcnYmc0X5a/yUR8HJk2Aykf3bb5jSKl76ulT9kBBy99zYhR3zjx4lPSiIwtNnMMbEkn/wILRKglUAACAASURBVMmf/O+6281csYKMn37CnHl9b/Xm7tpFxuLFpP/wfZXrFhw9iiEyksJTpzHGV92gKKVuqFvsermtDUL+kaM4+fpS969P4dGuHdnr15O5vOI3eP8MFJ49S9I77xL90MMkzPsHpvQ/3xbVxsRE8vbvJ3PZMix5pZd7vlEUnjmLcnfH/U5raqF7q9aI0YgxLu6m9Xk1uXv2EvPoYzh7edNi+XJ8hw0ja/36Ks9e848fBycnxGgkc0XV7uWcrVuJfnAasX95yuoGuUHkbNuOcnen1fp1tFq3ljpPPEH+wYMUhld/AyIRsb9ZXnD6dAWlK9Bv02YAsn79DUtRUZXqZqz4GeVpTcPN3rzluvT4I1CZPZV/UEolK6XOlpAtL7Gd5iWl1EmbvIVSqqDEsW9K1OmilDqjlIpQSn1m21cZ297Ly23yQ0qpFjf+NMum4OwZPDsEUu+pp2jy6f/w6t6d9CWLf/eR4c0ge8NGcHWl9qOPkhUcTOTwEeQdOnyr1aoSubt2A2DJzyfb9k97Myg4ewaPgACUqzXv2721NRe8unGE6pD588+41K5Ni19W4uHflpqTJmHJySF7U/krzJZFwbHjeLRrh3fPnmQsXYYYK7daa8aKFcQ98zfc27QBEdJ/rPr2nWUhIuRu3453jx44eXkBUHPyJJSHBxmLKr/I4tUYY2MxZ1nf+C44WX33k8VgIHfnTtxatsSSlUXu9rIXtCwLc2YmOZs2UXP8eNzvuouczTfvHv29qMwMIQhwSMkRkSli204T+AVYVeJwpFzZavOJEvKvgTnAnbZPcZuzgAwRaQN8ArxfrTOpIpaiIorCL+AReGX5itqPPIwpIZGcrdt+DxVuGiJC9sYN+Nx/Pw1eeZlWq1fh7OND8sc3d2/isjDExBA1YQJFERFVrpu7ezeujRrh1rIlmb9c3wqSRVEXyVy9ptRihWI2Uxh6Do8OV+4Dt9bWdYWKfqc4gqWoiNy9e/EZMABnH+vLXV7du+HWvDmZK1fayxni4kmY9w+KLpa9AqsYjRScPo1Xl87Uemg6puRkcrZUPGpN/+knLv/rdbx79aT54kX4jRpFxvIVVZ5VGhMTiX74EQwxMXZZUXg4xvh4fAZceanMpVYt/EaPJmvdOkwZ5e/VUEzi62+Q8MorDrJiN5+Tl5fVTVZN8vbvx5KbS/2XX8LljjvIXFX+khB5hw87XPusdesQg4GaD0ymxpDBFJw4gTGp8osg/hGpzJ7Ku4Ey7wzbKP8BYGlZx0uUuwOoISIHbHmwC4HiN4fGAsW7tqwEBqobkapRAUVhYWAy4RHY3i7z6d8f1yZNSF+06GZ3f1MpPHUKU0IiNUZYVwt1b9OG2jNnUnjqNAVnq75jWsay5cQ991y1dEmb/x1FoedIX+C4MY8pPZ3IkdYHT1lYDAbyDhzAp19fak6cQMGxY+U+CFM++4yEf7xa5jFTSgqJb7xB1OjRJM6bR8ZPjpufFIaeQ/Lz8bQFlAGcfX1xqVcPQ+TvM0PIP3gQyc/Hd+CVPQmUUtScPImCo8coioqyBptnPUbW6tXEzplb5sO68Nw5pLAQz3s649OnD65Nm5K+eEmpciUREdJ/DMKzSxeafvklTl5e1JnzOFJURPrC8reTLIuczZvJP3KE1C+vrI5aPOL26dfPoWythx9CioocDF6Z+plMZP/6K9m/bcCSf2WPgmI3n+/wYRScPo1YLNdo5Vo6b8HJ1xefnj3xGzuGvH37ynyoW/LziZ39OJcemEL+8ePW9c1WrMCjU0c8AgLwHTrU2t6WrdXS44/C9cYQegNJIlJyZ+qWSqkTSqldSqni984bAyUdsnE2WfGxWAARMQFZgOPi7DaUUnOUUkeVUkdTUkpvc1cVihcy8ywxMlTOztR6aDoFx47Zj/8Zyd6wAeXqis+AKw8Yv3FjUV5epR6IlSFjxXJyNmws9+3d8jClpJC1Zg3KzY2s9cH2KT5A+qJFGCIjufz22+QfK71Jev7hI0hBAd59+lBjzBhwdiarjNFb+uIlpH71NVmrVmFMdNz4J2//fiKGDiNz5S/UmjoV7549Sf7vh3ZXkDknh4RXXsHZzw/v++93qOvWuvXv5jLK2bYdJy8vvO67z0HuN24cuLiQ/uOPxMx+HFNyCg3++U9MycnEPfXXUv7u/GPHAfDsfI/1Xp4+jYLjx685CDBGR2OMjaXGyBFXXGatWuE7ZAgZS37CnFP5XfLyDlp3NMsKDsYQHW09t+078OjUEdf6jimZHm3b4nXffWT8tPSaLtrCc+ew5OYiRiN5h6+4PIvdfF5du2HJza1WVpgYjeRu24ZP/34oNzdqjh8PFgtZa0uv6Jt34ABiMKCcnYmZNZvUr7/GEBFJrcmTAes1c2vTmpwquvj+aFyvQXgQx9lBItBMRO4Bngd+UkrVAMoa8RfP3a91zFEo8q2IdBWRrvXq1bsOta1BV+e6dXFp6BjhrzlxIk5eXqQvqnh0VN5LfZa8PMRQ/raA1cVSUFBun3adLBayN27Cu08fnH197XJnX1/8Ro8m+9dfKzVNL8aUlkZRqHXLyZxtlfevAqQvssZjGn3wPlJYSNYa6wY25txcMpb8hHevXrg2bkTcs89iTHZc4TV39y6Uuzve996La/36+PTpQ9aaNQ4Pj9zdu0l65x08777bqt8Oxw1o0r7/AWc/P1oHr6fha//kjnffwcndnYSXX8FSVET8c89jiImh8eef4XLV/VScelrdFzdz9+3j8ltvk/D3ecQ98zfSvi87g0UsFnJ2bMe7d2+c3BxXKnWpWxffAQPI/HklRVFRNPn8c2o/NJ1G779PwYkTJM6b5zAyLjh+DNdmzewP35oTJuDk7U309OnEPf00WcG/llqjKXe3NaPIp3dvB3nduXOw5OSQ8dM1J/9XzsNkIv/IEXwGDEC5uJD67bcYk5IpPHMG3/4DyqxT+5GHMSUmkrO1/FF1/iGrkVFubuTt3WfvqzAkFI+OHfHsZF1C5FpprOXNHvKPHMGclUWNIUMAcGveHM+uXchatarU3z1nxw6cfHxouWY1bs2bk/rZ5zh5e1Nj+JU9O2oMGUL+sWPVyu76o1Btg6CUcgEmAMuLZSJSJCJptu/HgEigLdYZQclFU5oACbbvcUDTEm36UY6L6kZScPYMnu3bl3qRyNnXF7+JE8nesBFjUnI5ta2jzws9e3HpwWlkb9qMmM0YExO5/O//EH5/TxJff6N0nYMHSz20wDpSMaVeezONnJ07OX9PZ8K7dCVqzFjin3++zEyQghMnMCUlUWNY6Texa02bhhQVlTnSLvc8D1h3mnLy9SXnGgG3vIMHiRwx0j6KM+fmkbFsGb6DB1Nj2DA8O3WyjgYtFjKXL8eSnU29vz1Dk88+x5KbR/zzzzsEQPN27cbr3u442TI4ak6cgCklhZwtWzBcukTurl3EP/c87v7+NPv+O9xatCC3hMEypaWRd/AgfqNH49aiBQCu9evT8K23KDx7lovjxpO3dy93vPE63t0dl+IGa+qpJS8PU3L590B5iNlM4rx/kLlqFXmHD1Fw+jTJ//2wTF934ZkzmFNSHdxFJak94xGcatSg8X8/wKeXdWevGsOGUv/FF8j+bQMZi62BWREh//gJvO65x17XuUYNmv+0hJqTJ1Nw8hQJL75Iwrx5Du3n7t2DW4sWuDV13L7To107vPv0Jj0oqFSmU96hw1ya/pBDinZhSAiW3Fz8Ro+i5gMPkLV2HRmLra7X8s7Np29fXBs1IvPn8t1GeYcO49a6NV497iPPlg5bFBmFFBTg2SEQt5YtcPLzczAIhaGhRM+YScTQoYR17kJ4t+4YExJKtZ29aTPKywvvXlc2Fao5fgKGS5coOHHlbyUWC7m7duHduxeuDRvSfEEQPn37Uufxx3HyvrJQo++QIWCx/KljkNczQxgEhImI3RWklKqnlHK2fW+FNXgcJSKJQI5S6j5bfOARoHg/xnXADNv3ScB2ucnraZhz8zBERjkEEktS++GHwGwmfWHpDelFhLQfg4iZ/TjONWtiSk0l/m9/I2LgICKGDCVj2TJcGzYkOzjYYSQuRiMJL71Mwit/LzXVv/zW20QMGUpRVPnbNWYuW45z3br4TZyIa+PG5O7cRczMRzFd5TrL/m0Dyt0dn/79S7Xh4d8Wz65dyFi2rNI+17z9+3Hy87O60k6cKDfQmPr1Nxiiooid/TjZmzaTufJnLNnZ1Jn1GAC1pk/DEB1N7q5dpAUF4dXjPjw7dMDDvy13vP02BUePEf/Sy1gKCzFcuoQhOhqfPlcWTPPp2xfnOnWIf+55IocNJ3buEzh5edH0669w8vbGZ+AA8g4ftrs4sjdtArOZGiMd3wivMXQIfmPHYrh4kdozZ1JzUtlviLrbAst5e/chZnOZZcBqeK4edeftP4ApOZlG773Hndu30/rXYFzq1+fyv/9T6rrnbNsOzs749OlTZvteXbrQ9uCBUga+9qxZePfpTcr/PsWYlIQxOhpzWhqeXRwXbfTw96fhP1+lza6d1HrkYXI2bbY/HC2FheQfPoL3VbODYuo+8QTmjAyHB7aIkPzBBxQcO0Z2cPCVc7YNHLzuvZc6s2ehlCJt/ne4Nm2KW5uy3w5Wzs74jRtL3oEDZfrtxWgk/9gxvO/tjk+v3hiiozHExFB41hpQ9gjsgFIKz04d7cZWREh8/Q2Kzp/HM7ADNSdMwJKfXypWIWYzOVu34tO3D04eHna579ChOPn4OMRPCkNCrEbbFgdx9vOj6f/7hrpPOC7q6O7vj2vzZmQsXlRl9ypY42ZZ69eXGyv7PahM2ulS4ADgr5SKU0rNsh2aSulgch/gtFLqFNYA8RMiUvwEeRL4DojAOnPYYJN/D9RRSkVgdTP9/TrOp1IUnQsFEYdAYkncmjWjxrBhZC5d5uD3FrOZxL//neT338d34EBa/ryC1hs30Pjzz3D3b0utyZNps2kjjT/7FDEayV5/5R8mZ+tWTCkpWLKzHabI5qwsa7ZCfj7xL76ApQxXkyk1ldw9e6g5fhwNX/0HTb/+iqbfzceYlGRfg0dEKDwfTvamTfj06YOzT9lLTNeeNg1jbKx9tHUtRIS8ffvw7tED30GDwGIhd8fOUuUKw8PJP3SIOo/PxqN9e+KffZbUL7/Cq2tX+5Ted+hQnGvVIvHv8zCnpFJ3zhx7fb9RI6n/0kvkbNxIzMxHybS5lnz6XnlIKldXGn/yMfVfepE73nuXpvPn03LdWlxtLj/fgQPBaLSfV/avv+F+Zxs8/B23ngRo+MbrNP78M+q/9GK55+7u74/y8CDx1VcJ73E/cU8/Q86OHXZXgpjNpP3wIxH9BxD/omM7WWvW4OTnZ8+scfL2pv6LL1B49ixZqx1nZznbt+HVtSvONWuWq4tyKv1vqpSi4WuvIWYzSe+8a48feHXpUqpscRt1ZljHXRnLrJP6/CNHkcJCfPqUbRC8OnfGq2tX0n74we4Czdu9m8KQEJSbm4OhyDt4EHd/f1xq18a1QQNqTrYaWt8B/a+5nIff2LFgsZC9fn2pYwVnziL5+Xjdex/ettlR7t69FJw5g5OPD24trJvbeHbqRFFEJOacHHK3baPwzBnqv/QijT/6kIb/fBXv3r3IXPlLKXejOS2NGrZgcDHOPt7Umj6dnE2b7DGk3B07wckJ73KMdjFKKRq89BLGxMtEjRlLypdfOvw/i20V3YwVK0hfuKiUWypj8RISXnqZqOEjiBg6lMQ33yTpg/+S9O57JH3wXwpCqp4QUmXKW/Xuj/65ng1yUr//QUL9A8SYmlpumYKwMAn1D5DkL7+0y1K++X9W2WefV7jCZ9TESRI5eoy93KXpD8mFgYPkQv8BEv3oY/ZyaYsW29sM9Q+Qy+9/UFrfH3+UUP8AKbxwwUGee+CAnOvQUSKGj5CIYcMl1D9AQtu1l9x9+8rVy1JUJOG9esvFB6dVeA6FERES6h8g6cuXi8VikfB+/SXmL0+VKpfw+utyrmMnMaani7mgQGKe/IuE+gdIzs6dDuWSPvxQQv0DJGrS5DL7ztqwUc517CSh/gESMXzENXUrdV4mk5zvcb/EPf+CGOLjJdQ/QFK++qpKbVyNMT1dMoODJf7VVyW8dx8J9Q+QyHHjJeOXVXJp+kPWzXQGDpJQ/wDJP3lSRERM2dlyrmMnSXjjDUf9LBa5OGWqnL+/p5iys0VEpOjSJQn1D5C0BQuqrWPK199Yr9eIkXK++71iMV97dc6Yp56S8/f1EHNhoVx+5x0517GTmAsKyi2fs3u3hPoHSMbKlWKxWCTqgQfkQv8BkvqD9Z4sCAkRc2GhnOvQUS6/8469niExUaImTZaCsIo3lLo49UGJGDmy1D2R8vXX1v/T9HSxWCxyYeAgiXnyLxI1YaJcmjnzio5791rvt917JHLUKIkYNlwsRqP9ePaWLRLqHyDZ27aJiIjFbJbIcePlwuAhYjGUXiHVmJYm5+6+R+JffkVERCLHj5eLD06r8Dzs556UJHHPPS+h/gFyrmMnCe/VWyKGDbdvwFT8yT14yF7HYrFIxNBhEjVpsqQtXizRc+ZIWOcucq7T3RJ2T2cJDewgof4BEvOXp6QgNLTSupQF11jt9JY/2Kv7uR6DEPfccxLev3+F5WKeeFLOd79XzLm5kn/6jIS2D5TYZ5+t1HLP6UuXWh8Up09LQdh5CfUPkNTvvrM++APuEkNcnFgsFokcPUaiJkwUEZGEN96w3th79zq0FTluvERNnFRmPzm7dklYl65y6ZEZkr506TWNnF235csl1D9AsjY4LgNsMRgc/kHSFiyQUP8AKYqNExGRxLfelnOd7hZzfr69jCkry/rPM+8fV9oxmaQwvPQOU4a4ODnfq5fk7N5Trm75J09KeJ++kvL1NxWex9XEz/uHhHXtZjfcRZcuVbmN8rAYDJKxarVEDB0mof4BEta5i2T8skrMubly/r4eEv2odanm4mubf6r0Etz5p89IaMBdEj1rtsQ9/4JEDBnqcH2rpVdRkUSMGGl9WDzxZIXlc/ftsy7vvWaNRAwbLtGzZl+7fYtFIsePl4ghQyVn1y7rAGHZcjFlZMi5Dh0l8c03JffAAesDd/v2ap1D+tJl1mt21nHnuEszZ0rkmLH23xPeeEPO2R6OSR9e2UHQlJ0toQF3SeSYsbb7eoPjORgMEt6rt8TMmSsiIlkbNlivwdq15ep0+Z13JbRde8k7etQ6uPj22yqfV+7+/XL5nXcl4Z//lNhnn5W4l16S9KXLpCAsTM73uF9i5j5xpeyBg1bDu3p1mW2ZsrMl+fMvJKxrN+sgYvHiKutTjDYIV3Fh8BCJffqZCsvlnzhhvRm+/loihg6T8D59xZSRUak+TNnZcq7T3ZLwr9cl4V9XRtBFsXESGnCXJH/xhb399OXLRUTEnJ8vESNHyvmevaQoOlpErsxU0hZV/wa4GovJJJGjx8iFgYPEXGhdg92YliaRo0ZJ5OgxYsqy7v0QM2euRAwZaq9X/DApHmmJXDEaV/8zX5d+Fku19ljI3rrV/rCOmjT5hulTEovJJLkHDoghMdEuKx4t5x48JBcfnCYRw0eUq3+x0b8wYKBEz35cUr/7vsxyVSH30CHrgOP7HyrW32KRiGHD5cKgwZWenWRt2Cih/gFyvvu9Et6vv5iLrFuqxr34koR17WZ/eJpycipoqWxMmZlW4/Lv/9hl5qIiOdexkyT+54qs+O8b6h8gWZs2ObQROWqUdQY3fnyZs6SkTz6R0LvaSVFsnHVGPXKkWEymcnUyXE6Sc4Ed7KP6sgY410PyF19Y242IEBHrIDWs+73XnK2JWAdgyZ9+JkUxMdXu+1oG4bZby8icmYkxJgaPwLLjByXxvPtuvO69l5T/fYohOppG7793TV9vSZx9fakxdCjZwcFkrV9PjZEjcalVC7cmjfHucR9Zq1aTsWw5Tl5e1BhhDXw6eXrS5H//A5OJmJmPYkxIIGv1GnB1pcbIirdHrCzK2Zn6r7yMMS6OjMWLMWdnEzN7NoaYWIqiooh/9lks+fnkHTmCd88r+fle3bpZs422WbMoxGIhfckSPO+5B8/27cvrrur6KVWtZaS9778f5eGBJS/vhl6vkihnZ7zvu88euwCo9eBUa9D4rbcoOH4cv3HjytW/4b/+hf/pU7TZtpVm87+1B92vB+/u3Wm5ZjW1ple8d4VSilrTp2OMjbXW7VV2/KAkvoMH4dayJeasLOo8PtueHlu8vEb6Tz/hGRhof8u6qjj7+eEzYADZwcH2TLOCkyeRoiK8S7yb4XXvfeBiXbHfs6PjFpIetlhV/eeeKzPmUnPSJLBYiHv6aQxRUdR7+hmUs3O5Ork2qI+fLbPNtXHjcgPj1aXWtGkod3fSg4IwpaWRvWUrfmPHOAS4y8K5Rg3qPfN0qaywG8VtZxCKX9IpL6B8NcWZBLVnznS4OStDzUkTre8k5OdTa9qVf1a/CRMxxseTtXYtNUaNcggAu7dpQ9Pvv7MuY/zoo2StX49P3z641Cp//9fq4NOzJz59+5L69TfEPj6HogsRNPnic+54+23y9h8g5tHHkPx8vHv2tNdRrq629wHWEjF4CBcnTcIYHUOt6X+MvROcPD2tL5gp5ZAfftP79fCg7l+etL4c5eSE39gx5ZZVSpV63+BG4BEQgJO7e6XK+o0bi5OXF65NmuDWskWF5ZWzM/Vfegnv+++n5sSJdrlX9264Nm8GRiNePar2v1FKp7FjrBlNq1ZjTE4m/+BBcHLCq+uVfVycfbzx6twZl3r1cGnguG9C7UcesepYIoW0JG5NmuDdsydF587h0a4dvkMGV6hTndmPg4sLPgMH3PBNj1xq1cJvwniy1qwlbf53YDRSa8qUG9pHtShv6vBH/1TXZVQchCt2i1SGwsjICoN1ZWGxWCRixEi5OPVBB7m5oEDCunW3ulrOlO1qyTt+3Oov9Q+Q7C1bqtx3ZSiMiJDQdu0l9K52krXxyhQ8+dNP7QHq4gCovU5UlFx+512Je+FFuTRzpsT85Smx2FwIfwQKzp8v1w97M7EUFcmFIUPsfuo/OpnBwTfkvkr59luru+zAwetqx2IwyPlevRyCrsWxtZIURkZJ3rHj1eoje9s2e/C5shSEhVXbFVYRRRcvSmjAXRLqHyAXp0+/KX2UBXoLzSuY0tMpPHcOnxIj35uJKTUVnJ1LjfBTvvqKwpBQmn75Rbl1848dI3vDRhq8/BLqJowqAbLWB+Pk62PPsQbrIOHym28iBQU0ev93WWvw/wTmrCyUi4vDy0r/17EUFpKzdZt16YvrHEUb4+MpCA3FlJKCKSUFn549HWYINwLj5csO7r5bTdzTT5OzZSuN/vsBfqNH/y59XmsLzdvOIGg0Gs0fhaLISNIXLKTBq/+otMvvetF7Kms0Gs0fEPfWrbnjrTdvtRp2brugskaj0WjKRhsEjUaj0QDaIGg0Go3GhjYIGo1GowG0QdBoNBqNDW0QNBqNRgNog6DRaDQaG9ogaDQajQbQBkGj0Wg0NiqzheYPSqlkpdTZErI3lFLxSqmTts+IEsfmKaUilFLnlVJDS8i7KKXO2I59ZttbGaWUu1JquU1+SCnV4saeokaj0WgqQ2VmCEHAsDLkn4jI3bbPbwBKqXZY91pub6vzlVKqeNHxr4E5wJ22T3Gbs4AMEWkDfALo1dQ0Go3mFlChQRCR3UB6JdsbCywTkSIRuQhEAN2VUncANUTkgG351YXAuBJ1Fti+rwQGqhu9+LhGo9FoKuR6Ygh/VUqdtrmUitd2bgzEligTZ5M1tn2/Wu5QR0RMQBZQp6wOlVJzlFJHlVJHU1JSrkN1jUaj0VxNdQ3C10Br4G4gEfjIJi9rZC/XkF+rTmmhyLci0lVEutarV69qGms0Go3mmlTLIIhIkoiYRcQCzAe62w7FASU3+2wCJNjkTcqQO9RRSrkAflTeRaXRaDSaG0S1DIItJlDMeKA4A2kdMNWWOdQSa/D4sIgkAjlKqfts8YFHgLUl6sywfZ8EbJc/6649Go1G8yemwg1ylFJLgX5AXaVUHPA60E8pdTdW184lYC6AiIQopVYAoYAJeEpEzLamnsSaseQJbLB9AL4HFimlIrDODKbeiBPTaDQaTdXQW2hqNBrNbcS1ttDUbyprNBqNBtAGQaPRaDQ2tEHQaDQaDaANgkaj0WhsaIOg0Wg0GkAbBI1Go9HY0AZBo9FoNIA2CBqNRqOxoQ2CRqPRaABtEDQajUZjQxsEjUaj0QDaIGg0Go3GhjYIGo1GowG0QdBoNBqNDW0QNBqNRgNog6DRaDQaGxUaBKXUD0qpZKXU2RKy/yqlwpRSp5VSq5VSNW3yFkqpAqXUSdvnmxJ1uiilziilIpRSn9m20sS23eZym/yQUqrFjT9NjUaj0VREZWYIQcCwq2RbgEAR6QiEA/NKHIsUkbttnydKyL8G5mDdZ/nOEm3OAjJEpA3wCfB+lc9Co9FoNNdNhQZBRHZj3eu4pGyziJhsvx4EmlyrDaXUHUANETkg1j07FwLjbIfHAgts31cCA4tnDxqNRqP5/bgRMYTHgA0lfm+plDqhlNqllOptkzUG4kqUibPJio/FAtiMTBZQp6yOlFJzlFJHlVJHU1JSboDqGo1GoynmugyCUupVwAQssYkSgWYicg/wPPCTUqoGUNaIX4qbucYxR6HItyLSVUS61qtX73pU12g0Gs1VuFS3olJqBjAKGGhzAyEiRUCR7fsxpVQk0BbrjKCkW6kJkGD7Hgc0BeKUUi6AH1e5qDQajUZz86nWDEEpNQx4BRgjIvkl5PWUUs62762wBo+jRCQRyFFK3WeLDzwCrLVVWwfMsH2fBGwvNjAajUaj+f2ocIaglFoK9APqKqXigNexZhW5A1ts8d+DtoyiPsBbSikTYAaeEJHi0f6TWDOWPLHGHIrjDt8Di5RSEVhnG6oimAAAIABJREFUBlNvyJlpNBqNpkqoP+tgvGvXrnL06NFbrYZGo9H8qVBKHRORrmUd028qazQajQbQBkGj0Wg0NrRB0Gg0Gg2gDYJGo9FobGiDoNFoNBpAGwSNRqPR2NAGQaMpxmKB4wvBWHCrNdFobgnaIGg0xcQcgHVPw7n1t1oTjeaWoA2CRlPM5TPWn2mRt1YPjeYWoQ2CRlNMkm1TwHRtEDS3J9ogaDTF2A1C1K3VQ6O5RWiDoNEAWMyQfM76XRsEzW2KNggaDVjjBqZCqN8eCjIgX2/Jobn90AZBowFIsgWU242x/ky/eOt00WhuEdogaDQASSGgnMF/uPV3HVjW3IZog6DRAFw+C3XbQl1/QOk4gua2RBsEjQasM4SGgeDqAX5N/twGIS8VzKbq1U2NgIt7bqw+/9cxG6t/vf9gVGgQlFI/KKWSlVJnS8hqK6W2KKUu2H7WKnFsnlIqQil1Xik1tIS8i1LqjO3YZ7a9lVFKuSulltvkh5RSLW7sKWo0FZCfDtlx0KC99ffaLW/ty2nGAog7Vr26piL4vDPsfLfqdUXgl8dg6VQwFlav/4rIS4VjCyAn6ea0fyv4aQr8PKPicn8CKjNDCAKGXSX7O7BNRO4Ettl+RynVDuueyO1tdb5SSjnb6nwNzAHutH2K25wFZIhIG+AT4P3qnozm/yCGfDAZbm4fyaHWnw06WH/Wbn3rZgj56bBgDHw3AGKPVL1+0lkozILjC6wj16oQtQMST4EhFy7urnrf1yLmEKx8DD6+C9Y/Azv+ff1t5iZDwsnrb8digSP/n73zDm+ruhvwezQs771HYmc4O85OIBDC3oGwdyirDdCWttDSAm06KKtA26+MUkaBMkpZSRhhBAhhxdjZy1mOHe89ZVnrfH8cXVmyJO9M630eP7aP7jiS7j2//bvPqeyy/tJQDHtXw+6PobNt8HM5zPQqEKSUXwLdc/AuAF50/f0icKHH+OtSyk4pZTGwB5gjhEgDoqWU30r1EOeXuu2jHetN4FTNejjo1O6C166CD+6Cdc8MXCsLcvB4/gx4746De44ql/HrthBGQUfDwBaIwdBSAS+cA5UbQW+CTa/1/xjaAtleC7tW9W/frx6HyFQIiYKd7/X/3IGo2QkvnAV7PoVZN8LYM2Hb8sE3EVz9e/j3uYNXGEq+gvd/Dt8+2f99t/xP/XZYh16IHgYGGkNIkVJWArh+J7vGM4ADHtuVucYyXH93H/faR0ppB5qBBH8nFULcIoQoEEIU1NbWDnDqHmx9C4reh42vwYd3Ka1s3xeDP+6RgKUF/j4d/vcDaKk83LMZGE0HVH+hrW9DZ+vBO0/1FghPgKhU9X/CaPX7UFoJTaXw/JnQfACufhMmnA/b3lYuoP5QsQHC4iAqTXVu7SvlhWpBO+42GHsaFH2oNOehoOB50Bng9kI4+0E47lbobFbnGAxlhcqaqRyklVDkEpxb31Rus74iJWx6HbLmQUikshKOcoY6qOxPs5c9jPe0j++glM9IKWdJKWclJSUNcIoeVG6EpPHw6wPw8x0QkQzf/GPwxz0S2PqmWtB2rIR/zIbvnlbVuEcSvQXiNI3L3gE7Pwi8nb0TOpoGPo/qbZAyGTTDNH6U+n0oaxG++iu01cKSlTDqJMi7Qlko/V1kKjZC+gyYdrXSyFsq+n7+0BiYeT2MOxfaa6C8oN9vwwdru1o0J14Aka57NvtEiM5Q4wOlsw3qitTf+wcRBJcSdn0IhjB1v1Ss7/u+5YUqPXn6NTBqIez+pH8C5QhkoAKh2uUGwvW7xjVeBmR5bJcJVLjGM/2Me+0jhDAAMfi6qA4OFRshbZpaCKLTYc7NsOcTqC06JKc/qKx/SVXd3rYOsmbDql/B538e+LHeunlg+1rb4YsHlV/bk7YaeHgUfP9s4H2L10B4IsRkdZnm3ZES3rgOnju997n4u1kddtWyImVy11hctvp9qALLdquyBiacBxkz1Niok5WC4rloSql8/IE0d1uHioekT1eLlHTCxld6P3/dbqU4zL4JQqNh7OlKox+I26j7Z7z1bWUNzLqha0ynh6mXKYHV1gdLf+/narH1pGqLen8I2P91/+epUbdbCYKT7gJ9CGx5s+/7bnodDKFK2I09QyUmaPGoo5SBCoQVgBZWXwIs9xi/wpU5lIMKHue73EqtQoh5rvjAdd320Y51CfCZK85wcGmphLYqdfNozLpB+W6/G4Av8UiicrNyHcy4Trk/rnkbxp8HBc8NzG+78TXY8sbAXE/fP6cyXtb903t8w8tqoVj9R/9tIqSEfWsgZwFMvhj2fqYyVLqz6yPlK6/bBc1lvq8DtFYp19kjfoLF9XtUy4pUD4FgDIPoQ5h6uucTZQ1MvbxrTG+AKZeq96d9Pp/+Dv65AD79rf/jVG0F6VDXdHyO+uzWv9y76+fbJ8Bggrk/Uv+HxSotvierzB/V29VnvNlDeBc8D0kTYMRx3ttOvULNdWsfFuCPfgMrfuItbCo2qN8TzoMD6/ofQNcocr3HqZerRX3rW4Et6e3Lu1zKdqvadtw5LiF6hho/yt1GfUk7fQ34FhgnhCgTQtwIPAicLoTYDZzu+h8p5TbgDWA7sAq4TUqpfbpLgWdRgea9gOZAfA5IEELsAX6OK2PpoKP5HdOndY1FJELe5Uryt9cfkmkcFNa/pATb1MvU/0LAnFvUorPtnf4dy2Hv+qz6a5o7bF2C4PvnuoJ/TqdKPUwYC50t8OVffPet26UE9qiT1MIoHbD9Xe9t7J2w6m7lMwco/c77dadTWSD/mA0731fC8L2fdy0sUsLn94PO6Ltgxef0LhCkhNV/6PJBD5RNrytLaNTJ3uN5l4PTpr6zb/4BX/8N4nLgm/9TQro72iKpXdPTr4OmEiVMe3oPRR+qCu3I5K7x8edC/W6VeNFXdr4P5np490ew+1M1n4r1StHqnieSPF5Z570FzjualAXXWqGuCc/3GpUOky5yxRE29X2enuxaBalTVO3JlEugrRr2f+W7naVZZUm9dIH6velVlXigCfHoNHWc7pbMUUZfsoyulFKmSSmNUspMKeVzUsp6KeWpUsqxrt8NHtvfL6UcLaUcJ6X80GO8QEo52fXa7ZoVIKW0SCkvlVKOkVLOkVIeGrWsYiMInfoSPZl3q9IYC57v/RhHor/Q1gGb31A9ecLju8ZzFqhK3J5cNP6o3Qk2s/p735r+7bt9uTKjZ92oFvcdK1zH+UwtVAvvVr7u/Gd83TPauXJOUtk/SRN8zfnvnoTGYlj8jArqdRcI3z0J7/9Cacy3fgun/0GlVm5+Q72++b9qTqfcowSAJ/Gjem9fsfZR9fPlI76vOZ0qBrHzAyXwti/33QbUgrdrlVqM9Abv11KnQvJEWPMQfHyPck3ctk5p7yt/CmXdfPwVGyAiSfnnQQWmI5LglUvghXPVNd09NbJmh/puRp/iPa618OiP26h4jbrGkifAG9fCx/eBMVwJNn/kXakWcq3LrD/KC3CHFPd+3jVesUF9r9knqP/9LeK9YW5Q1sW4c9T/uWep68ife3LPanDalQDYsVJ9/uGJMObUrm3GnqGuwcHEsw4zw7dSuWKDalMQEuE9njwBRp8K3/+r5wyP6m3w2ESVttpY0jVu61Ams7+L3OnwH0htqYSSb3ueb3s9fLpMpcfu/Uxl4PhzBWxfoVwxM67zHhdC+YjLC6G8P4Ez16KTMkXd8D0JQc/XpFSuiPjRcPbD6ve6p9VrBS+orJ4J58Mp9yrf7afLvI9VvAZiR6iFWgi1YJZ+q7JxQH1max5RAdDcMyBztq9A2PqWCrBet1y5zmbdoLb76NdqIfrgLmUZHP8T3/eSMFppu4Fu7l0fwWd/grB49Zl294WvuB3+Pg1evxI++6OKcxS+6Huc7e+qlMWpfhZNIdR4W7USAhf9S7l2LntJZUS9frVyh2loi6SmjRtD4ebPYOGv1THe+xksv837HPtci2x36yQmU2nwO9/3//67Y7PAgXy1KF79lrI29q9V7r7QGP/7TL5YxSp6shIOfK8Ut+iMrrlaWpT1kj5dnScxF0o84gi1RfDubcrNtOo36jr0d6/s/ljFIXJdJVHGMOVa3b7C997ftUpdsxc+BUu/UdstuAv0xq5txp6hLNl9n3O0MjwFgpTKDeLpLvLkuNvUDbT1Lf+v1+xUxUNOm/IpPjEX1jysFrXHJsDbN6mLsTtv/gD+c5Hv+Ee/hn+foxaWQHz/rMoT//AueHkx/HUy/DkdnjoB3lgCn92vNN/8Z5RbYeQJvsfIuwKMEcp901fKCyE0FmYuUSmRjQEyb4pWqSDxxlfV/wfWKXfBcbcqzXfuD6Hse7XAFH2oLAODSS1s83+qNHVNy3M61GKSs6Dr+JMvVr/f+zn891p49lSlsZ15vxofcVxXURYogVGxXrk+tAVSp4fz/6a2efZ0tRhc+JQa746WaeTv/dbthrduUtblla8BUsUBNNrrlBto8sVw46fwy2KlZLx3B+zopnFvfkO5zjxjWZ7MvhHO/DNc8ar6vEBZfle+plyAmiDVsm66Hyd2BCz8Fdz+vXIb7nzfO2az9zN1/tgsfJi0WCkEVVu9x/d+Bs8s9BaWZfng6FSCKyoFrn1HLZrze6ghiUyCMacpBSqQ3/7AOmUl5Z6lWmrYrV3uIe29jpyvFCqHXS3kbyxRQfpdq6DwBRWD2POp77GLPlR1F2ke68CUS5VC5RkLcNjV/2PPUNdK4li44hWY9yPv42XMUvdKf2MvnrTVKEt4xU/guTNhw3+GLv23DwxPgdBaqRb8tAACYfQpykXx7ZO+GnHtLnjxfHVh/GAV3J6vsjI+v1/5eLNPUBkeZfmqL4xGc5nSPIrXePumO9vUYiqd8M7SwC0Dtr+rFr1fFMGS9+C8x9ViEZMBVZth7V/g7ZvVDTzjOtD5+WpDY1RcYeubfe/3X1YIGTNVWh0Edht983flU313KSy/XQmvsDjlFgD1OyQK3r5FaVEzr+/a9/jbVWbPf69RgcnKTWrRzlnYtY0WJN3zqbLO0qfDJc93uXpGzANkV3XvLpe3cvy53vNMmaQsAkcnnPWAr6vIfT6XQHjrZnjlMnj3VjW/Z0+HZ09TmuEVr0DWXJXzv+ujrn23vaPe44m/UBle4fFKq0+frvzPOz+A5nKlyZZ8rayAQLWYpiiloIRG+76PeUuVdl2xsSvrJpBgEQKmXaWUGM19Ze9UGTqjT/a/z4zrlALx7RNdY04nfHSvskY8XSvFX6pusSOP7/r8rngFEsf4P7ZG3pUqPuCvqMvpUApJ1hx1T9ralVLRPVaSfQJYW133waNQuwMueQHu3AW/KlGa/YaXvY9ttyo3UO6Z3vfKqIUqocDzPZflK+GrWRKB0Btg8kUqAePT3/e8kPuztPd9oRTKt25U15C5Xll0z546sKr1AWDofZNjEK2as6eb57hbYcWP1YU66iQ13lYLLy0CJCx5v+tiv/xltYiFxSmNrKVSacqbXoNT71PbbHwVty90y1sqzQ2UFmPvgBPvVIv65/fDGX/0nk9tkUpnO/thpVFHpULOid7b2DuVz7r5gLdm3Z3ZNymtacPLSjP35JPfKhP+VFcWS2eburkmnAcJY9TCV7wGZv3Ad34lX8Mp96l4w9pH1fiJv+hyyYVGw/Srldto1MKu4i9Q21z7LrxwtgrajXPdeN3fxzXvqAXNGOb7vjJnqQWp9Nuuwqq4bFVn0p1T7lX++LS8wJ9T0ngVT6otUgpE9Va1OEemqIVh7g/Vdw1Kc9z2jgqi641Kw0ue2FX5DGCKhKv+p4rPXr/S+1xTLw08j5448efqe/z43i6ffyAlR3stYaya36wfKBebvcM3fqARHg8zrlUW5an3qdTsHSugZpuKDRS+qK4nIdR9kj7dV3D1Ru5ZSlHZ9JqvYKrdqZIOMueo613olTumfi/EjFBJINAVR8h/RgmpqZd3XUOGEPV//r+U2zXCVfO6Y4USIt0VBr0B5v8EPvwllHyjBFzRhyrxINDn5MnZDyvB/NVjKgi++J/KJdhSodzIxWvUZ2Uzw9Jvu2ozpFRWflQ6XPpvdW0KnXo/n/wWnjtNCejTfu8dGxxihqeFUBkgoOzJlMtU0EjTFKRUgSRzgzKHk3K9t0/L61ogotOUT3bzf5WW4HSqGzdnAYw4XmkQmoaw9W210J58j9Kav/k/1ffFk23vAgImLAo8X4NJZW6MPb3LteCP1MlqQf7qce/WDOWFysL5+m9djccqN6mLO2OmuulzTlJme3fNp+AFdcPMWKKEyVX/g9yzu9IYNebcooJ287r5sUFp6tetUOdb/5Ky0KJSvLfRG/wLA1BCJS1PLXKdbcqSGXeuf81bp1faZU8dUnR6ZUFc+zb8aC38fLsK6C5ZARf9s6teAJSW2dnSFeM48J2KeXQnIgFu/Fhpr+f9VQW5L/pXV91DfwmNUfGB/WtV4WFUmrr2AiGEshBLvlIW677PlQKQ7ce9qDFvqbJ21j2tvvc1Dymf/WnLVIV3xQb1eZcX9qyIBMIYqjKFdqz0rUY/kK9+Z81R7zVjpgosV2zwdvdGpSqFZdNrSik760Hv40y7WikS7jYTdlWTkzwJxvipX5l+rbr3NcVm1yrInt83Yac3qu/2rIdUSuuDWfBwDjw9X7mSd6xQykJHo3cDwpKvlSUy/yeQOVNd6zqdCsj/uFBZtRteURlzm984aAktw1MguAPK4YG3MYaqQrXdHymf8abXVJuLU+/rWZBo5F2ptPWSr2D/l2qhmLFEaYN1u5SJb2lWvudJi9WXf8afVBHWu0tVUzcNzV3U083eH874k/L/rnlY/S+lyggJjVF+ea3lgRZQzpipfo86Ccx13sU3tg6Vgjfh/C5tJ/cMuOp17zRGUFbBr8vU6/5IyoXr3lWB2ok9CL9AjDhOzXnXKuUS0rTmg03OSSowvuujrriTFvPoTni8civM+oGy0LTU4IEy8wdqgW4uDWzxeqIJqi1vqsU1c7ayfAIRl62sqYIXVJFbzXY46VdK6zaEqSZ6pd+p62YgAgHUvWIzK6HgyYF85e7R3HejT1ZxocZi3/c6cr76fe6jvhp06mSlLGz8j/p/02sqg+yUe/y7VkPClYdgz6dKYavbpRScviKEii9ct1wt5Gc9CJe+CD/8UsWTrnpdJTgUvqDikQBrH1MZYdOv8T2eKVJ5DX64BuJGKtfwN//X9/n0g+EnEKR0lff34eaZdaPK5//kd/Dhr5R2P+/Wvp1n/LnKZ77xNbXAhsaqINvEC5VWtuUN5Ut2WJWGBOrGvPAJdbFqwULNXTTpwoCn6jepU5T5mf+MinMUfaA0lFN/qyybwheUFlVeCLEju0xz7YYv9ogjbHtHCTbPStSe6K1vYeoUFSc56Vf9f18j5qmU4TUPq8+7e23BwcIUqYKpuz5SC23W3IFr/f1Fb4DTXS7G9Bk9bwtqcc2crb7jyk19c4Mc/2NlAb13h1KkJi1WxWuTLlTvt+gDZSFmzR3Ye8iaoxIhumcbleWrY2rXzOhTXNXJ+N6/J/xMJQhMvMD/OaZdo5SwskJl5aTP6Eo39cfsm8AUo+Jh0OWC6g85C+D03ysra9KFSihpCQwn3a3Wh0/uU+vR3tUqVhTIAgZ1b9z4iRJ6067u/3z6wPATCC0Vqk9LoAwjTyKTlMlW9L4rI+VJ/xkp/ggJVxfB9uUqs2Tq5crqCI9XmRVb3lImbMwI5f/WyFmgXC35/1RBpr64iwbCKfcqDe+jXysfZWKusmBm3wQt5UrLLiv0nltMpkof9QwAFryg/NI9uR36iyGk75+zJyPmqd91RcqN0z2v/2CSe6ZKhazeqjJVDiW5Z8JlL6skg74w5TJo3A9I33RTf2TMVBq40w4n/bLru5mxRBWFFf5bLeo9Wdw9IYSyEorXqnRqUP7++j1KeHnOI8RlzXS/f+NzVNA8EFMuUVbcG9cpy/2Ue3tWTkJjlIfA1q7cl0Mt4CMSYMEvVPbS27co4TOrD9+fTq/uUS0WMsQMP4GgVd32FHzz5LjblV/y7IcDZ6QEIu9KdUE5OlVwTmPKpSqzYu9qJTS6X5in/k4tsu/eqiyJoXQXaUQmd12Q9XuUP1tvVEG+6ExY86AqKtPcRRqjFiph8Y/Zqh1EWb6KfRyijuU9EpmsfMlw6NxFGlrrAqFXVuChRAjfQsSemLRYzTM0pm+WMiiXxdylal+NEfOUxSAdA3cXaUy9DJCqyM/eqbKJwNvq0BthzCkq4K9Vp/eV8HhlEbSUKeHWF8to3lIwRQe2OgbLHFdiQl0RzLmp/wH5g8DwyzKq2tp7QNmTpHFw556BaZsjjlOmcFis9/nGna3S+Wzt/n3NIeEqO+G509XNNueW/p+7L8xdqgJVsVldKXV6A8y6XhVdgcqt9mTh3SrV9cD3ynUUGtOzZnaoGXm80jJHn9r7tkNJfI6qLI7O6IqlHKlEJimXYUhE36/rjJm+yoEQ6jgf36PiKIMhPkfFQwpfUEHy+NHKtdpdYJ3/d+UWHAizb1J1GKf+tm8KTEQi/HRTzzGWwWAMhbMfURb63KUH5xz9RByKPnIHg1mzZsmCggG055VSuURiMnvfdihoLFGaTXS69/jKnyqXzI/WBr44P39AZf38ZMPQWwgaVrMypT0XhtZqeHyScpP9pjywX1NK5UbwrNY83LTVqM88a3bv2w41HY3Kl26KPPTnPlzYrapQLffMobES96xW8TqtEvmWLwZ/TE9sHT376YcBQohCKeUsv68NO4FwpOB0qB9DSOBtpARLU//N46Fg5U9VZtS1/WyGFyTIYLFbVUZQYu7QxqaCAD0LhOHnMjpS0Ol7D5wKcXiEAahc6iMhLhBk+GEI6XvWWpAhZfgFlYP0jaAwCBJk2BEUCEGCBAkSBAgKhCBBggQJ4iIoEIIECRIkCDAIgSCEGCeE2Ojx0yKEuEMIsUwIUe4xfo7HPr8WQuwRQhQJIc70GJ8phNjieu3vrucuBwkSJEiQQ8iABYKUskhKOU1KOQ2YCZgBLUfxce01KeUHAEKIicAVwCTgLOBJIYSWZvMUcAsw1vUzgMYhQYIECRJkMAyVy+hUYK+UsqSHbS4AXpdSdkopi4E9wBwhRBoQLaX81vWc5ZeAQ1z7HyRIkCBBhkogXAF4tiq8XQixWQjxvBBCS6TPAA54bFPmGstw/d193AchxC1CiAIhREFtba2/TYIECRIkyAAZtEAQQoQAiwDteXpPAaOBaUAl8Ki2qZ/dZQ/jvoNSPiOlnCWlnJWUdIT3iwkSJEiQo4yhsBDOBtZLKasBpJTVUkqHlNIJ/AuY49quDPB8kncmUOEaz/QzHiRIkCBBDiFDIRCuxMNd5IoJaCwGtrr+XgFcIYQwCSFyUMHjfCllJdAqhJjnyi66Dlg+BPMKEiRIkCD9YFC9jIQQ4cDpwA89hh8WQkxDuX32a69JKbcJId4AtgN24DYppcO1z1Lg30AY8KHrJ0iQIEGCHEKC3U6DBAkSZBgxbLqd2mw2ysrKsFgG+ACNIENCaGgomZmZGI1H0HMSggQJ0ivHlEAoKysjKiqK7OxsgsXOhwcpJfX19ZSVlZGT089HjgYJEuSwckz1MrJYLCQkJASFwWFECEFCQkLQSgsS5CjkmBIIQFAYHAEEv4MgQY5OjjmBECRIkCBBBkZQIBxjLFu2jL/85S8+4++++y7bt2/v9/H279/Pq6++6v7/3//+N7fffvug5hgkSJAjk6BAOAzY7fZDfs6eBEJP8+kuEIIECXLsckxlGXny+5Xb2F7RMqTHnJgeze/On9TjNn/84x955ZVXyMrKIjExkZkzZ3LnnXeycOFCjj/+eL7++msWLVrEtGnTuPPOO7Hb7cyePZunnnoKk8lEdnY2BQUFJCYmUlBQwJ133skXX3zBsmXLKC0tZd++fZSWlnLHHXfwk5/8BID777+fl156iaysLJKSkpg5c6bXnL755htWrFjBmjVr+NOf/sRbb73FjTfe6DWfLVu2cN5553HJJZcAEBkZSVtbG3fffTc7duxg2rRpLFmyhLi4OCoqKjjrrLPYu3cvixcv5uGHHx7SzzlIkCCHh2NWIBwOCgoKeOutt9iwYQN2u50ZM2Z4Lc5NTU2sWbMGi8XC2LFjWb16Nbm5uVx33XU89dRT3HHHHT0ef+fOnXz++ee0trYybtw4li5dyubNm3n99dcDnhPg+OOPZ9GiRV4Lvud8AK6//nq/53zwwQf5y1/+wnvvvQcol9HGjRvZsGEDJpOJcePG8eMf/5isrCy/+wcJEuTo4ZgVCL1p8geDr776igsuuICwsDAAzj//fK/XL7/8cgCKiorIyckhNzcXgCVLlvDEE0/0KhDOPfdcTCYTJpOJ5ORkqqurWbt2LYsXLyY8PByARYsW9Xm+2nz6y6mnnkpMTAwAEydOpKSkJCgQggQ5BgjGEIaQ3tqARERE9LqdwWDA6XQC+OTym0wm9996vd7t+x9omqc2n+7nlVJitVoD7hdoHkGCBDm6CQqEIeSEE05g5cqVWCwW2traeP/99/1uN378ePbv38+ePXsAePnllznppJMAyM7OprCwEIC33nqr13MuWLCAd955h46ODlpbW1m5cqXf7aKiomhtbQ14HM/zLl++HJvN1qf9ggQJcuwQFAhDyOzZs1m0aBF5eXlcdNFFzJo1y+1a8SQ0NJQXXniBSy+9lClTpqDT6fjRj34EwO9+9zt++tOfcuKJJ6LX63327c6MGTO4/PLLmTZtGhdffDEnnnii3+2uuOIKHnnkEaZPn87evXt9Xr/55ptZs2YNc+bMYd26dW7rYerUqRgMBvLy8nj88cf783EcddS1dbLwkc8pqgoKwCDDk2Oq2+mOHTuYMGHCYZqRoq2tjcjISMxmMwsWLOCZZ55hxowZh3VOh4Mj4bvoL1/truOa59bx8CVTuWxWMCau575yAAAgAElEQVQS5Nhk2HQ7PRK45ZZb2L59OxaLhSVLlgxLYXC0UtWiYjYN7YHjJ0GCHMsEBcIQEyziOnqpDgqEIMOcYAwhSBAXmkCoa+s8zDMJEuTwMCiBIITYL4TYIoTYKIQocI3FCyE+EULsdv2O89j+10KIPUKIIiHEmR7jM13H2SOE+LsItssMchgIWghBhjtDYSGcLKWc5hGkuBtYLaUcC6x2/Y8QYiJwBTAJOAt4UgihpdE8BdwCjHX9nDUE8woSpF9UtSjLICgQggxXDobL6ALgRdffLwIXeoy/LqXslFIWA3uAOUKINCBaSvmtVClPL3nsEyTIIaPGZSHUtwUFQpDhyWAFggQ+FkIUCiFucY2lSCkrAVy/k13jGcABj33LXGMZrr+7j/sghLhFCFEghCiora0d5NQPLXfddRfjx49n6tSpLF68mKamJkB1Ew0LC2PatGlMmzbNXY8Q5NDicEpqWpWFUN8ejCEEGZ4MViDMl1LOAM4GbhNCLOhhW39xAdnDuO+glM9IKWdJKWclJSX1f7aHkdNPP52tW7eyefNmcnNzeeCBB9yvjR49mo0bN7Jx40aefvrpwzjL4Ut9eycOpyQjNgyLzYnZOjzbcbz87X5+/t+Nh3saQQ4Tg0o7lVJWuH7XCCHeAeYA1UKINCllpcsdVOPavAzwrPbJBCpc45l+xgfHh3dD1ZZBH8aL1Clw9oO9buavHfWdd97pfn3evHm8+eabQzu3IIOiullZBRPSoilv6qC+zUp4/PDLyl65uZLCkkYeuHgKJkPvlfJBji0GbCEIISKEEFHa38AZwFZgBbDEtdkSYLnr7xXAFUIIkxAiBxU8zne5lVqFEPNc2UXXeexz1FFYWOhuR/3222/z/fff+2zz/PPPc/bZZ7v/Ly4uZvr06Zx00kmsXbv2UE43iAstw2hiejQA9cMwsCylpKiqFYdTsq+2/XBPJ8hhYDAqUArwjitD1AC8KqVcJYT4HnhDCHEjUApcCiCl3CaEeAPYDtiB26SUDtexlgL/BsKAD10/g6MPmvzBoLd21Pfffz8Gg4Grr74agLS0NEpLS0lISKCwsJALL7yQbdu2ER0dfcjnPpzRqpQnpqnPveEojiM88MEOzpiUysyRcb1v7EFNayfNHaqpYVFVKxPSgtfgcGPAAkFKuQ/I8zNeD5waYJ/7gfv9jBcAkwc6lyONQGUUL774Iu+99x6rV692b6M93wBg5syZjB49ml27djFrlt9WI8OOa59bx8T0aH599sHti1TTYkEnYFxqFHD0Zhq1Wmz888t91LVZ+y0Qdno09SuqDjb4G44M60plp5RYbI7eN+wHgdpRr1q1ioceeogVK1a4rQeA2tpaHA41h3379rF7925GjRo1pHM6WpFSUljSyJqig59RVtViITHSRHKUEs6H02XkdEp3Cmx/qWpW+20ua+r3vkVV6pGzaTGhR0XH15e/K2HJ8/mHexr83+rdPL3Gt4Pw0ciwFgjNHTZ2VbcOaSFSoHbUt99+O62trZx++ule6aVffvklU6dOJS8vj0suuYSnn36a+Pj4IZvP0UxLhx2z1cHumrYhF9zdqW7pJDUmlPAQPSaD7rAVp1ntTm57dT3zH/qMAw3mfu9f6RIIe2rbaOvsX6bUzqpWUqJNzM6OP2gCwWy189CqneypaRv0sb7cVcuaXbX9fp9DiZSSF78t4c3Cst43PgoYfmkUHnTa1RPCyhs7CNELIkONQ3Lce+65h3vuuQeAZcuWAbgfhtOdiy++mIsvvnhIznusUd7UAagagaKqVvKyYv1ut760kfLGDs7PSx/wuapbLGTGhSOEIDHSdFhcRhabg1tfWc9nO1Vi3sYDTWTFh/eylzeahSAlbC1vZt6ohD7vW1TVyrjUaMalRrFiUwWtFhtRQ3RPALR12rnhhe/J399Ae6edP1wwOC9xab0SmHtr2gJeGwebimYLdW2dNHdYcTglet3R3XVnWFsINrsTg06HyaCjpMF80LXQIP2jsrnD/ffWiuaA2z35+V6Wrdg2qHNVt1hIjVHuoviIkENenGa22rnxxe/5vKiGZedPxKgXbKto6fdxKjw+sy1lgT+z7tgdTnbXtDE+NYpxKSqOsmsI4wjNZhtXP7uO9aWNpESb2HSg/y4tT6SUlLosqKGwNgaK9j5sDkl5Y0cvWx/5DG+B4HASYtCRnRiOQLC/vh2Hc2gfGLRs2TKvGoQgCovNwWVPf8t3++oDblPh0naNesHW8sCLW3FdG/XtVjqsAxPoFpuDRrON1OhQQAmEQ+0yeunbEr7eU89fLsnj+vk55KZEsa0HIRiIqmYLSVEmMmLD2NSPOEJJgxmr3UluSpQ7sF5UNTQLrdXu5OrnvmNHRQtPXj2DC6dnsL2yhU77wBWwujYrHS4Fbk/t4RcIAPvqDt88hophLhAkRr0gxKAnIy4Mq334VqgeajYeaCJ/fwMfbqkMuE1lUwdGvWDWyHi2lvvXlu0Op1tT9NSO+0Otq2VFsksgJESGHHKX0dd76hiXEsXFM1WN5uT0GLZVtNDfJxpWNltIiwllamYMm/thIWgxg/GpUWTEhhERoncHmQdLQUkDW8tb+PNFUzhjUip5mbHYHJIdlQO3QEobuuokDqeFsPFAE5lxYQAU1x39tRvDViBIKbE5nBj16iMID1FVmRab83BOa9hQWNIIwKYeFq2Kpg5SotXiVlTVitXu+91UNFmwOdSiOVCTXatB0CyEhEPsMrI5nBSWNDJ3VFcywaSMaBrare4gcV+paraQGh3K1MxYShvMNPbR0tlZ1YpOwJjkSHQ6QW5qlFca6mD4ancdep3grMmpAG5//0AyoTQ0JSA3JZK9h0kgOJySLeXNnDI+mahQQ1AgHM04pMQppVsgGHQCg04Myow9VrHaHbQPcSbHepdA2F7Zgs3hXwhXNFtIjwljUkYMVoeT3TW+C5SnmV7RNDCBoFUpp7hdRqZD2s9oS3kzZqvDKwA8KT0GoN9xhMrmDtJiQsnLVPtv7sHV5klRVQvZiRGEGpViND41il3Vrf22UPzx9Z46pmfFEmlSOSzpMaEkRprYOIg4Qkm9GSFg4bhkt7troNS1dXLiw5+RX9zQr/321rZhtjrIy4xlVGLEMVHdPWwFgt2lVRr1KitACIHJoKfzGLEQpJQcaDBjHoKFvLqlk/31Q3exSylZX9pIXLgRq90ZMMWxsrmDtNhQJrvaSWzz4zba76GVlQ9QIGiZOakeLiM4dMVp6/aphWhOTpeFMCEtCiHoMXbSnfZOOy0WO2mxYUzWBEIfF92iqlbGu2IHALkpUTSabW532kBpNtvYXN7MCWMT3WNCCKZlxQwqsFzaYCY1OpSJadE4nHJQ1+eaoloONHS4s7v6iibQ8rJiGZUUGbQQjmasLq1UsxAAQo06LHbHkGhF3Vm2bBkZGRnuNtcffPCB+7UHHniAMWPGMG7cOD766KMhOV+rxU6j2UqjefCLWqfdicMpsQfQ5PtLcV07jWYbV84ZAeDX1+10SqqaLaTHhpGdEEFEiN5vplFxXTtRJgPpMaEDFgg1rZ2YDDqiw5QGmxChBEJfAsvf7KnzEkoDYV1xPWOSI0mMNLnHwkMMjE6K9LIQzFY7j31c5G4v0R3NvZQWE0p0qJFRSRE9uuQ8j1vSYCY3pUsguAPL/cg0klL6+PO/2VuHlHDCmESv8bzMWPbWttNi8f9eeuNAg5kR8eGMSY4EBhdHWLtbFT72R/iCCihHmQyMSowgJzGC8qaOoz5TcdgKBJsfgWAy6tXCN8SZRho/+9nP3G2uzznnHAC2b9/O66+/zrZt21i1ahW33nqru3J5MGiCwDzAzBtPNHM8kGunv2jxgwunZxAbbvTrS65r68TmkKTHhKLTCSalx/i9YffVtZOdGEFGXFiPMQQpZUB/elWzhdSYUHc7kXiXQOgtjlBU1cp1z+fz6Ce7etyuJ+wOJ98XNzBvlG8x4qT0aK9Mo9fyD/D3z/bwv4IDPtuCr6WTlxnbJz/9npo2pMTLQtBST/tToFZY0shpj61h5aauZsVf7akj0mTwqRPQ/u9Lauz/Cg7wn+9KvMZK6pVAGJUUATDgOILTKflqTx2gYhr9UQY3lTUxNSsGnU6Qk6jmMZSW9OHgmC1Meyj/IXY27Az4utXuxOZwEmHq+ggcTtXKItSo91tgMj5+PL+a86tez+2v/XUgli9fzhVXXIHJZCInJ4cxY8aQn5/Pcccd1+t5AmF3Ommx2NEJgcXmHFTBjMPpxO5UgsBqdxIWMuBpuVlf2kR0qIExSZFMyYjxq8VWuLVdlcExOSOGV/NLfN7L/vp2pmfFoRNQWNoY8Jx/W72bJ7/Yy5d3nUxqTKjXa9UtFlKiusY0Tb0nl5HTKbn33S3YnZLdg8jX31bRQrvVwdwc3wKyyekxLN9YQX1bJ3HhIbz87X4APthSyU0n+rY30eo2tM9sSkYM72wodwu8QGjB43GpXc3sEiJNJEaa+hVY3uHa9vFPd3HOlDT0OsFXe+qYNyreS/ECmOpyaW080MT8btZDd577qpi6tk6unjsCIQQdVgc1rZ2MiA8nPMRARmzYgFNPd1S1UNdmZdbIOApKGiltMDMyIaLX/Sw2BzsrW7llgfoeNIFQXNvO+NSjtyngsLUQJL5N6HSu/52DcBn11P76H//4B1OnTuWGG26gsVEtXuXl5WRldT0mIjMzk/Lyct/5uvou9UWDaTbbkFKSFGVCIt352gPBM1hnHSILYX1JI9NHxKHTCfIyY9lV3epTQ1Dpcv+kxaqFbHJGNBabk30eN36n3UFZYwfZiRGkx4ZR2WTxW0eyuayJ//tsD1a7k4+2Vfm8Xt1iIcVjwYzvg8vozcIyvt/fyIj4cPbVDbx+ZV2xqsOYG8BCACU01uyuZX+9mSkZMawvbfIbQNcshBRXgV1eliuO0IuVUFTVSqhRx4huVdFaYLmvFLuCqvtq21mxqZwDDWZK6s1+F/zY8BCyE8J7jSM4nJLiunbq2qzsdR3/QKPKMBqRoOY7JjnSy2UkpaS1j66otbuVdbB04WhABfj7wvbKFuxOydRMZeloAmHfUR5HOGYthN40+X21bTglbh8kqAtpe2ULMWFGMuP61zJAI1D766VLl3LfffchhOC+++7jF7/4Bc8//7zfBd5ft9SWDhslDWYiTQbSY8Pc2SD+aDLbMBn0JESEUN1iwWy1uzM8+kunp0AYYCaHp4baYrGxq6aVc6emAUpTdDgl2yubmTmya1HU4gEZsV0WAqiK5bEud0ZpvRkpYVRiBO1WO3anpLa100sbttgc/OKNTSRFmjAZdazaWsWS47Pdr0spqW7p5LQoT/+96mcUqMFdfVsnf/5wB3Oy47loRgZ3v72F8sYO9wLVH77b18CoxAiSo3w1eM9Mo/ziepKiTPzl0jzO/OuXrNpaxQ0n5HhtX9FsITEyxP1gm4lpMeh1gk+2V3P6xJSAXXh3VbcyNjnKx4oclxrFK+t8rbJA7KtTlc5CCP726W63FXPiWP8WQF5WrDugHojyxg73NZhf3MCY5Eh3ywpNgI1JjmRdcT1Op0SnEzy7tphHPiri3zfM5vjRPVsfX+2uIzclkhPHJhGi17GlrJnzpvbeAkUTZNNcrq8Ik4GUaNNRH1gethaCVpTmiRCC0H5mGkkpaTJb2VnZ4k5f9HfjpaSkoNfr0el03HzzzeTnqy6NmZmZHDjQ5RMuKysjPd33gjTbHAgEHTbV7K2yuQOnH6200+ag3WonLsKIQa/DZNBj7hy8hWAy6LE6+q8Ff72njnkPrOapL1Q3yI2lTUgJM0ao1syaL3nTAW/NrLLZQphRT0yY6qUzKjGCUKPOq0BNu/lyXBYCQHmTd0O4xz/dxe6aNh68eArnT01nXXG9l+bfYrHTYXN4CZHe+hk98OFO2ix2/rR4MmNTXEHN2r5p0g6ndH+mDqfk++IG5gboNxQTbiQrPowPt1byxa5arpozgnGpUYxPjeIDPwV9Vc0dXu8jLETPNXNH8L/CMh5ctTOgdbmzqtUdRPZkYpqvVdYTxXXtjE6O5GenjWV/vZlHPy4iJdrE6KRIv9vnZcZS1WJxWzb+2Otxbs2aKnHVIGiunTHJkVhsTsqbOjBb7Tz5xR6sDidL/7O+x7l3WB3k729QwsCgY0JaVI/FfBc9+TXzH/yMu/63iZWbKkiJNnl93jmJEUGBcDTSvSjNE1M/Mo06bQ6K69opbTCrh7S3dDLn+Pl+219XVnbdwO+88w6TJ6vGXosWLeLV116jpKaZ4uJidu/ezZw5c/ycy4nJqCM3JYrYMCO1rZ3srmnzqQ9oNCtTOc7l7A8P0WPuo6vJ7nD6ZBJZ7epzCjXqBmQhvPytCgY+8tFOvtlTR2FJIzrR5c5IiQ4lJdrk49bQUk414WrQ65icHuPV6kK7+bITI8h0C4SuxWVDaSP/+nIfV87JYuG4ZM6anIpTwqc7qt3baEJcq1LWUO0rfIPK3+yt483CMm5eMIrclCj3Ytc9y8Vic7Cvto09NW3srm7lgy2V/PyNjcy+/1Om/v4j7vrfJv5XcIDWTrvfgLLGpDRVcawXgqvnqqysc6akUVDS6LOQVjZbSI0O8xr73fmTuGbeCP65Zh9/eG+7z3XQ0G6ltrXTHUT2xNMq6w2r3cmBBjOjEiM4fWIKUzJiaDTbOGFMUkDLxK0M9ODS0gTC/DEJrNvX4E6njjQZiAtXyoI706i2jVfXldJotvHXy6eh1wlufLGApgCZdvn7G7DanW4LZnKGSlzwp2jVtXWyvrSJ8BA9H2+vZn1pE7Oyvb+3UUmRfRaeRyqDeYRmlhDicyHEDiHENiHET13jy4QQ5UKIja6fczz2+bUQYo8QokgIcabH+EwhxBbXa38Xga6gIaJ7UZonoYa+ZRrZnU721LbRYXWQHhtGbmoUeh2kZE/gsssu82l//ctf/pIpU6YwdepUPv/8cx5//HEAJk2axJnnLeaE2dM486yzeOKJJ9Drfd1BFpuDUIMOo15HVnw4oxIjkEj21rZR3mimvq2TurZOGs1WIk0GjIauCmy7w9mnDKHSBrO7AlSj0+EkRK8jxKDD6nD2KwujptXCpzuquXruCEYlRfLj1zawemc1uSlRXl00p2TE+hRQVTSpojRPzp6SxraKFvfiW1zXTkJECDFhxi4LwSPT6H+FZUSEGLjn3ImA8slnxIbx0dauOIL2rIWR3fznqsGd90JisTm4552tjIgP56enjgWULzwxMsRHINz2ynpOeXQNpz22htMf/5JbX1nP6h01LBibyAV5Gby/pZK731bP/PYXUNaYnBHtfu+a0Dpniqr4XbXV20qoalFtKzzR6QR/vGAyN8zP4YWv9/PkF959+7UYQa4fC2F0UgQmg85v/Ud3ShvMOCWMSopACMHPz8gF4KRxSQH3mZQejUEneixQ21vbRkJECGdNSqWqxUJZYwelDWay4sPdgmaMSyhvr2jhX2v3MW9UPBdOz+Cf186kvLGDpf9Z7zfGs3ZXLSF6nfvzn5oZQ2un3W2BeKLFFv5wwWQ23Hc6H/70RO6/0Ltb66jECBrNtj5Xhx+JDCaGYAd+IaVc73q2cqEQ4hPXa49LKf/iubEQYiJwBTAJSAc+FULkuh6j+RRwC/Ad8AFwFkPxGM0A2LoVpXliMqqFtNPm8CswNJrNNhxOyZikSMJd/vnUmDDKGs3c+rNfcu+99wJd7a9ffvllv8eRUnLD7b/gult/xsj4cGLCfdN4HE6J1eEkztj1WmSokbHJBqpbVPtdT7TFEbpacpitDkJ6eGi60ylpdwV2nVK6A+xWu5NIk4EQvQ4pJXaHxGjom7x+s7AMu1Nywwk5/GB+Nov+8TVby1vcmq5GXmYMn+6opsViI9olKCqaOljYbTE5Py+N+9/fzvKN5fzijHEUu1JOQflwY8ONXsHW/OIGZmbHueMnQgjOnJTKf74roa3TTkOblcc+2cWp45PdWS8aCX4W+Sc+30NxXTsv3zjHK4YzOinSHfAElZ77zd56TpuQzKJpGQiUJTRjRCwG1zV13/kTWb6xHHOno8cMoLmjEtDrBDfMz3aPjUmOIjclkg+2VHH9fBVH6LA6aDLb/B5LCMF9501ga3kzH2yp5LaTx7hf0wTCeD8CwaDXMT41qk/V0l3uO7U4nzwumfd/cgITesi4CTXqmZAWzcbSHgRCTTujkyKZ41q01xU3UFLfztjkrvnGRYSQEBHCv9buo8ls49FLpwEwOzue3y2ayD3vbOWbvXWcONb7elq7u47ZOXGEue4RzSLaXNbkDhJrbHW5kiZlRKPTCb+PF3VnGtW3ExcxBOl4h4EBWwhSykop5XrX363ADiCjh10uAF6XUnZKKYuBPcAcIUQaEC2l/FYq9fMl4MKBzqsv+KtB0NBudEsv7pEGs5VQo959MQHEhRuJCDFQ1dzR5yIuq70rrbMtQM2A1k6jeyBZrxOkx4YxMS2aCWnRTEyLZnJ6jNvvDqq2QidEr/UIHS63kpQSiyYYnMq1ZjIoCwH6nmkkJfz3+wPMyYlndFIkY5KjeOjiqQAcN9pbI57qch1oN53V7qS2rdOdPqmRHBXK/DGJLN9YgZQq+8Tzxk2PCXMHo+vbOtlT0+ZV/Qtw1uRUrA4nn+2s4TfvbEGvE/xp8WQft0ZCt46nu6pbeXrNXi6anuGzsIx2Zblo1tO2ihY6bA4WT89kUV465+elMycn3i0MACJNBq6eO5KbF/T8dLzZ2fFs+O3pTB/h/TjMc6ak8X1Jg/vJal0pp/6FixCC2Tlx7KzyzugqqmolJszoflJcdyZlxLCtorlXy7DY1UIkxyNlc1K6ytHviekjYtlU1hQwS2tvbRujkyMYmxxJbLiR7/bVc8BPAH90ciRNZht5WbHMH9N1fV08I5OIED0fbPHOLqtusVBU3er1XeamRBFi0PmtjdhS3kxOYoRbYfGHZ+rp0cqQxBCEENnAdGCda+h2IcRmIcTzQgjtSs4APCtqylxjGa6/u48fNGz2wALBoBPodaLHwHKH1UGH1UF8RIjXQiKEWqAdTlX9Ct7tryubOnyegqVp5SF6XcB+QVrDvVCD/6/LoFeuJINe53MD6oQgzKjvVSC0e/Tt0eakLf4hBh0hrs8qUBxBSkmnR6zCandQUm/mqjld1sD5eems/eXJnDM5zWvfqS7NLH+/yjipbrEgJaTH+i5uF0zLoLTBzNd76qlp7fQSCBlxYW4L4fv9Kq13Tjc/78yRcSRGhvCn97bz1Z467j57vI/gAdXPqMPmwGy1Y3c4+fXbW4g0GbjnXN9nO49JiqS5w0adKwhd4Hofs7P790zjQPhbhM6dkoaUsMJVBFbVrW7DH9Oy4nA4pVdMoKiqlXEpUQH9/JPSo2mx2CnrpXGg230X3r8H6kwfEYvZ6vCb3trYbqW+3croJNVwb052PB9tq8Jqd/qkyGpxhNtPHuP1XkKNek4en8zH26q8hM47G1Rq92kTkt1jRr2OiWnRflNPt5Y3uy2IQGTFh6PXiaM6sDxogSCEiATeAu6QUrag3D+jgWlAJfCotqmf3WUP4/7OdYsQokAIUVBbO/Dn7NocEoF/l5HW08jSQ5O7RrMVIQSxYb4Xf1iInqhQAy0dNi+tSkpJg9lKk9nmZT2YrXb0OkFcRAgWm8OvZWGxOdAJ4dbS+0u4SU+HzdFjfYW504HJoCdEr3M3ddPS/UIMOndMIpCF0Gi2UVTdyv569aChdquDmDCju8OlRlZ8uI/QiosI4YQxibyWX4rV7vRoweC7uJ05KQWTQcdfP1XVwV4CIbarWvn7/Q2EGHRM6eYK0usEp09Mpaa1kzk58V4CyxPPfka/XbGNwpJGfnf+JBIifTVpbTHSAqD5xQ2MTAj3CVQPJWNTopgxIpZX1pXidEqvthWB0FIkNReNlJKi6lZyU/1nAYEqjoPe2zrsq233cbP0helZSmhu8OM20j5PLXA/JyeeVou6NrsLhEtmZnLTCTmcOj6Z7pwzJY36dqu7eZ3TKXl1XSlzsuMZk+ztKpviJ7Bc39ZJRbOFKRk9F5wZ9aqWozeBYLU72VreTHlTR58SNbRAur9g91AzKIEghDCihMErUsq3AaSU1VJKh5TSCfwL0FJmyoAsj90zgQrXeKafcR+klM9IKWdJKWclJQUOVvWGzeHEoNcF1IpCjbqAFoJTShrNVmJCDV4uAE+iQg1YHU6vHH6z1YHDKZFIWj0sAbPVQXiIwe3nbvejyXfaldtmoLH28BC9lyuoO1JKzFY74SF6wkMMbmvC6hKKIQYdOiEw6gNnGrVabOh1ArPVzu5qFWxfPD2jx3oJT248MYfqlk7e31Lh1vI9YyEaUaFGTpuQQoGr/YWXyyg2lNZOOy0WG9/vb2B6Vqw7J9+Ty2ZlMioxggcvmhLQpaH1M3rkoyJeXVfKj04azYXT/Ruuoz366UgpKShpZNbIg/9c7GvmjaS4rp1v9tZ3tfDuQSBoD87RgrhVLRZaLXa/GUYa41JVfUJvcYTu7ru+MjIhnLhwIxv8VJl3FwiewffuAmHGiDjuPW+i3+9z4bgkQo06PnQF4b/eW0dpg5mr5/kqA1MyY2i3OrwKzDSLYUpG74/pHJ8axRdFNaza6lsAqfHXT3dx3v99xfwHPyP33g855dEvKGv0DWQ3d9h44etiTn1sDSc+/DlLXsinvu3gtmUfTJaRAJ4DdkgpH/MY9/QHLAa2uv5eAVwhhDAJIXKAsUC+lLISaBVCzHMd8zpg+UDn1RcCpZxqmAx67E7fFExQBWIOp+wxaBQVqhb3NkvXwt9qsSMAg05Hi6s5mcPpxGJzEB6iYhE6Ify6jbR2GgMlPETNp7q1k2az1ed9ddqd2J2SCJOecJMem8OJ1a4EmmoLrj6rkAACQUpJm8VOTJiR3JQo4iKM6IRasPrKwtwkxiZH8iZgoe0AABBQSURBVK8vi90PuvHnMgK4YFpXnUZ2gqeFoBaJ3dWtbC1v9okfaEwfEcdndy5kVID8eOiqVl6xqYJzp6bxyzPHBdw2PSaU8BA9e2ra2FvbTkO7lTk5Q+Mu6olzpqQRF27kP9+VUNncQVy4sdfrZNqIWLdA0PoU5fYgEEKNesYmR/aYetrWaVfuu6T+CwQhBNNHxPnNNNpb247JoCPD9QCaienRRJoM6ATusb4QHmJgYW4yq7ZWua2DuHBf6xW6Wmp4pkFr1tGkXiwEgPvOm8iYlCh+9J9CHvhgh8+91mqx8fK3JZw4NpEHLprCHaeNpaalkzte3+i17Ze7ajn+gdX8fuV2okON/Oik0awrbuCcv6/td5vu/jAYC2E+cC1wSrcU04ddKaSbgZOBnwFIKbcBbwDbgVXAba4MI4ClwLOoQPNeDmKGEfgvSvMk1JVp5K+rZEO7lRC9rsfK3xCDHpNB72UJtHXaCA8xEB1moNVixymlWxMPdwmD8BC9j0CwO1XKqJb9NBCMeh3x4SG0u1Lqtle2UNPalcOuuYjCQwweWUl2rHanV2aSlnraHbPVgUNKokwGjHodmXHhpMWGeVWB94YQghtPyGF7ZQvLN1QQE2Z0C7LuLByXTEyYkbSYUK+gviZAVm6qxCkJKBD6QpIryDpjRCyPXprXY3BUCOHKNGpzxw+656gfDEKNei6blcUnO6rZdKCZ1B7iBxrTs2Ipb+qgptXi9tv7K0rzZGJ6dI8WgtbtddQALARtTrtr2nzut701beQkRrirpPU6wdyceEYmRPSo0Pnj7CnKTbhqWxWfbK/mkpmZfq3HsclRpEaHumMM0LeAskZ6bBhv/HAe184byT+/3MeNLxZ4xS5ez1e1J3eeMY4r54zgjtNyuX/xZApKGvn7Z3sA5e685eUCsuLDWXn7Cbx723zuPns8by89njCjniv/9V3ABoeDZcBpp1LKr/Dv///Az5i2z/3A/X7GC4DJvnsMPVpRmqbF+yPSpFw4FU0WjHod0WFGpJTUtVlp67STEh3aq/smKtRAQ7sVp1PVPNx24xLKS/aiA+obG0mIj+OTtesoP1DK3LFzGTduHHanZELeDF7793Nud1SnO6A8cAsBIDM+nHQp6bA6qG6xUNOiGqYZ9TraOx0YdAKTQYcEd1aS1e50p9SCEgg2s9MrLRWgtVNZPxEDbI+hceH0DB75qIii6la/aZCe8/jZaWN93Gua1rhyUwV6nXBXQw+EzLhwnrhqBvPHJPTJOhudFEF+cQP5+xtIiAgZ8OLYX66aO4J/frmPLeXNfv3n3ZnmURm+s6qVlGgTsX5SnT2ZlB7D2+vLqWmx+I2L7OuWctpftAyqzWVNXlk/e2vbmNQtkPunxZPdcYT+cMr4ZEL0On7zjmpIeGWA2JFeJ7j2uJE88lERu6tbGZsSxdbyFmaM7Pu1ZDLo+eOFk8lNieS+5dt4es1ebjt5DDaHk+e/LmbeqHiv7q8XTMvgy111/OOz3cSHG3n0412kx4Tx8o1z3YoJqLTYlT8+gd+v3N6v+fSHYVep7HAGLkrTEEIwMkG1SihtMNPWqbIsKps7iAkzevWtD0RkqAGnlLRb7bR12nnkqefJLyhk48aNnHbOIs44ZxFmqwOTQcfo0aPZuHEj3+UXct8Dj3tlBGn91UMHYSFo6IQgwqS6Q0rZ9SxhLY4hhHBnJbV32t0ppxpappGtm9uozWInLCRwTKWvhBr1XHuccjNl+IkfeHL9/ByvfHqAxAgTIXrVg2hyevSgBdS5U9N6XSw1xiRHUtFs4avddczKjhtwvKe/jEyIYEGuWkR7ih9oTM6IcRWDNbKrurVHd5F7H48me6Aymt74/oA7yFlc244QKh4wEKZmxSCEd2DZYnNQ2mD2aXuRFhPWpzl3JyrUyIljE2ky2zh+dEKP7sIrZmcRYtDx4rf7qW/rpLypo9eAsj+umTeS8/PSefyTXWw80MTKTRVUNlv44YLRPtv+/oJJjEyIYNnK7USHGfnPTd7CwPN9/OXSvIDtQAbLMdvcrurPf6Zzh2/7a4eU6K0OWox6zL3kSBsAu9VBiSs7J3LiBEb87t5eb3at/XVcchqpyclMzpvGVTfdTphL0/zkvXd57o2VmK12L5dHeIge4YojRLsymDrtTndAd6gwGfXERRipb7cSG26k0+4gLqLLHA436d3CwjOzybMWweR6L3aHkw6rnaQhyqi5dt5Invpi74Aaxel0gvTYUPbXm5l9CFw2nmjusZrWzkN+7mvmjuDLXbU9ZhhphBr1jE+LorCkkd3VbVzbhzjPRJdA2FrezIS0aC5/5ltK6s1YHU5XYLuN9JieGy72RHSokbHJkV6B5ZJ6Vfk8egBxiUCcMyWN1TtruHpuz+85IdLEBXnpvFVY7i6I6y3l1B9CCP504WQK9zdwx+sbCDHoGJcS5VNwCcor8cRVM3jsk13cc+4EvwkVh4JhZyFomZd9UeB0KM1crxOEGvXuBbsnPNtf//PFV9mwvoBOu5PIUKWBr127lpSUFDJG5uBwSsKMeoqLi5k+fTonn7yQbeu/o82jJkALKA+1xqkVImmtKiI8/PWevvsQva+F4BlYbu+0I4GoQWrjGgmRJlbcfgI/OWXsgPbXbqTZg4gfDATPeMmhFginjE9m6cLRnDMlrfeNUW6j/OIGOu1Ovy0ruhMVaiQ7IZyv9tRx1bPfUd9mZVJ6NA+t2klNi4V9de3uB9UMlOlZcWw40PWAmu4ZRkPBhdMzeOEHs92tP3piyfHZdNgcPPShUioHIhAAYsKMPHb5NEoazOyqbuPmBaMC3ssT06N5dsmsAWVrDRXHrIWQ+pvf+B3XTMDM1OgB5/X3hGf76/QkPQtOOxunU7oXzNdee42rrrwSgUAiGTUik9LSUhISEigsLGTRBRfwv0++ITHC5KpNcBLdQ7xjoIQY9KpfT1snQggfS0XD02Vk0AuEEF6B5dZOO3pXQHyo6C3I2ROaq+lQL8oj4lXw02TQuZ9jcKgw6HX86qzxfd5+WlYc//muFKDHlFNPJqXH8P6WSsJD9Lx4wxzVX+hva/n9e9sprm3nohmDqyWdPiKW/xYcYH+9mZzECPcT0AYraDzR6wQnj+s9zgJKAMzJjid/fwPZCeF9CigHYt6oBO46cxyfbK9mUV7vrbUPJ8esQAiEECro01OW0eDPoY7tGbiODDVgt9t5++23KSwsxGrSY7E5iYoIQ0Qq98jMmTMZO2YMNWX7iYyKxu6U2J1d7pmhJjnKRGO7lTBXewsNo6uZncPh3QdfCOGVeqqlm2rWz5HAFXOyyE6McKeNHipCDDrGJEWSHG0adCzlYKMFloXA3b67N2Znx7F6ZzXPLpnlFra3nzyGxz7xLRAcCFpgOb+4nuhQA9sqWsiIDQuYaXYouH5+Nvn7GwZsHXhy68Ix3LpwTO8bHmaGnUCIjzARH9F7UHigLFiwgOuvv567774bm83Gl6tXceW1N2DU61i16mPGjx9PZmYmnTYHdqekrq6O+Ph49Ho9+/btY/fu3ZwwfRIt0uDuTzMUAWV/GPU6RiSEY/ATS4kJM2K1O30W+hBDl0DotDuxOpwkmQ7e59lfZo6M93rQzqHk6WtnHrTvaigZlRhBVKiB+IiQPi+4S47P5pJZWV7p1j88aRTLN5azt7adnEG6dsYkRxJlMvCrt7YAqgvsyT10Sj0UnDExhVPGJ3P+Ea7VDyXDTiAcbGbMmMHll1/OtGnTGDlyJCctONHdbO7111/nyiuvBFRg1wSs+vJLfvvb32IwGNDr9Tz99NMkJiYQ75Tsr2+nvdM+qKK03ghkCgfqiROiF7RZHGyraHbnV0ceBJfW0cjh9P32B51OcNH0DML6oX0LIXxqb0wGPQ9fkscf3ttOXubgtGi9TvC3K6exs6qViBADESZDj8+JOBQY9Dqev372YZ3DoUb0p7/9kcSsWbNkQUGB19iOHTuYMMG3+djhZNmyZURGRrob3PUHp5R02pxe/v3Djdlqp77Nik4n0AtBqFHnNzXzSPwuggQJAkKIQinlLH+vBVW7Ixhdt2DvkUB4iIHw+OBlEyTIsUjwzj7IaA/ICRIkSJAjnSM/AtZPjlYX2LFE8DsIEuTo5JgSCKGhodTX1wcXpMOIlJL6+nr+v727C5GyiuM4/v2xbk4akRaF7UhusFQWlBFhL0RkkFq0XW4geNFlkEUQilfdR9RFBWEvUqEXJrV4EYUF3Vn2Qmyt5palU1u7Eb3QRRr9u3jOxbDtlOvs+Ljn/D4wzDxnZnb+P2b2/HnOMy+NRu9+C8DMeiOrJaNms0mr1aKbH8+x7jUaDZrN5v/f0MzOKlk1hP7+fgYHB+suw8xsQcpqycjMzE6fG4KZmQFuCGZmlizYTypLmga+Pc27XwT8NI/lLBQl5i4xM5SZu8TMMPfcl0XErF8UtWAbQjckHez00e2clZi7xMxQZu4SM8P85vaSkZmZAW4IZmaWlNoQnq+7gJqUmLvEzFBm7hIzwzzmLvIYgpmZ/VupewhmZjaDG4KZmQEFNgRJ6yUdljQhaWvd9fSCpJWS3pM0LulzSVvS+HJJ70g6ks6X1V3rfJPUJ+kTSfvSdgmZL5C0R9Kh9JzflHtuSY+k1/aYpF2SGjlmlvSipClJY21jHXNK2pbmtsOS7prr4xXVECT1Ac8AG4DVwP2SVtdbVU/8BTwaEVcBa4EHU86twP6IGAL2p+3cbAHG27ZLyPw08FZEXAlcS5U/29ySBoCHgBsi4hqgDxghz8wvA+tnjM2aM/2PjwBXp/s8m+a8U1ZUQwBuBCYi4uuIOAHsBoZrrmneRcRkRHycLv9ONUEMUGXdmW62E7ivngp7Q1ITuBvY0Tace+bzgduAFwAi4kRE/ELmuam+qflcSYuAJcD3ZJg5It4Hfp4x3CnnMLA7Iv6MiKPABNWcd8pKawgDwPG27VYay5akVcAa4ABwSURMQtU0gIvrq6wnngIeA/5uG8s98+XANPBSWirbIWkpGeeOiO+AJ4BjwCTwa0S8TcaZZ+iUs+v5rbSGoFnGsn3fraTzgNeBhyPit7rr6SVJ9wBTEfFR3bWcYYuA64HnImIN8Ad5LJV0lNbMh4FB4FJgqaRN9VZ1Vuh6fiutIbSAlW3bTapdzexI6qdqBq9FxN40/KOkFen6FcBUXfX1wC3AvZK+oVoKvEPSq+SdGarXdCsiDqTtPVQNIufcdwJHI2I6Ik4Ce4GbyTtzu045u57fSmsIHwJDkgYlnUN1AGa05prmnSRRrSmPR8STbVeNApvT5c3Am2e6tl6JiG0R0YyIVVTP67sRsYmMMwNExA/AcUlXpKF1wBfknfsYsFbSkvRaX0d1nCznzO065RwFRiQtljQIDAEfzOkvR0RRJ2Aj8CXwFbC97np6lPFWql3Fz4BP02kjcCHVuxKOpPPlddfao/y3A/vS5ewzA9cBB9Pz/QawLPfcwOPAIWAMeAVYnGNmYBfVcZKTVHsAD/xXTmB7mtsOAxvm+nj+6gozMwPKWzIyM7MO3BDMzAxwQzAzs8QNwczMADcEMzNL3BDMzAxwQzAzs+QfO2S1sTeoVaQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "pred=test(data_ct,tr)\n",
    "model=load_model(\"./hopefully_model.hope\",custom_objects={ 'loss':mloss(0.8),'score':[score] })\n",
    "pred2=test(data_ct,tr)\n",
    "pred=(pred+pred2)/2\n",
    "y=tr.FVC.values\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "sigma_opt = mean_absolute_error(y, pred[:, 1])\n",
    "unc = pred[:,2] - pred[:, 0]\n",
    "sigma_mean = np.mean(unc)\n",
    "print(sigma_opt, sigma_mean)\n",
    "\n",
    "\n",
    "idxs = np.random.randint(0, y.shape[0], 100)\n",
    "plt.plot(y[idxs], label=\"ground truth\")\n",
    "plt.plot(pred[idxs, 0], label=\"q25\")\n",
    "plt.plot(pred[idxs, 1], label=\"q50\")\n",
    "plt.plot(pred[idxs, 2], label=\"q75\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T23:06:54.594731Z",
     "iopub.status.busy": "2020-09-27T23:06:54.593286Z",
     "iopub.status.idle": "2020-09-27T23:06:54.597794Z",
     "shell.execute_reply": "2020-09-27T23:06:54.596777Z"
    },
    "papermill": {
     "duration": 2.870107,
     "end_time": "2020-09-27T23:06:54.597918",
     "exception": false,
     "start_time": "2020-09-27T23:06:51.727811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/sigma_opt_unc_sigma_mean\",\"wb\") as f:\n",
    "    pickle.dump([sigma_opt,unc,sigma_mean],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T23:07:01.211897Z",
     "iopub.status.busy": "2020-09-27T23:07:01.210020Z",
     "iopub.status.idle": "2020-09-27T23:07:01.214072Z",
     "shell.execute_reply": "2020-09-27T23:07:01.213591Z"
    },
    "papermill": {
     "duration": 3.582281,
     "end_time": "2020-09-27T23:07:01.214246",
     "exception": false,
     "start_time": "2020-09-27T23:06:57.631965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8943.199, 18623.967, 16833.719],\n",
       "       [ 8924.906, 18608.805, 16816.238],\n",
       "       [ 8920.842, 18605.434, 16812.354],\n",
       "       ...,\n",
       "       [ 9433.291, 19108.154, 17343.42 ],\n",
       "       [ 9408.9  , 19087.938, 17320.111],\n",
       "       [ 9376.381, 19060.98 , 17289.035]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.035621,
     "end_time": "2020-09-27T23:07:06.927837",
     "exception": false,
     "start_time": "2020-09-27T23:07:03.892216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 2745.207745,
   "end_time": "2020-09-27T23:07:10.986266",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-27T22:21:25.778521",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
